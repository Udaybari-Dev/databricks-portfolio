{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3106342-e416-4870-ba62-3737662cffc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862407c9-431a-49f8-960b-6a01d9b155fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f72de72-d1f7-41cb-ba3d-f134fc67eefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"101\", \"2023-12-01\", 100), (\"101\", \"2023-12-02\", 150), \n",
    "        (\"102\", \"2023-12-01\", 200), (\"102\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "\n",
    "### Prodcut_id-  Duplicate entries \n",
    "# latest date - timestamp coloumn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b216ed8e-2901-43a1-bbd7-8a5173764138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Casting date col  string to date format  , DateType isApi in pyspark or sql \n",
    "df = df.withColumn('date' , col('date').cast(DateType()))\n",
    "\n",
    "# or we cam doo --- df = df.withColumn('date', to_date(col('date')))\n",
    "\n",
    "#Drop Duplicates , accending[ 1,0] bcz prodcut id in asscending order and date in desscending order\n",
    "\n",
    "df = df.orderBy('product_id' , 'date' , ascending = [1,0]).dropDuplicates(subset=['product_id']).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f36d38-4bb0-4716-aed3-a1215b784899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. While processing data from multiple files with inconsistent schemas, you need to merge them into a single DataFrame. How would you handle this inconsistency in PySpark?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27555080-5f81-4d6e-a34d-c5e9d160e279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mergeSchema\n",
    "\n",
    "df = spark.read.format('parquet') \\\n",
    "    .option('mergeSchema', True) \\\n",
    "    .load('/FileStore/Data/Datafiles/*.parquet')\n",
    "\n",
    "# Don't Use When:\n",
    "# 1. Performance is Critical\n",
    "#     mergeSchema is SLOW!\n",
    "#     Spark has to read metadata of ALL files\n",
    "#     For 1000 files, reads 1000 schemas!\n",
    "\n",
    "# 2. Schema is Consistent \n",
    "#     All files have same schema\n",
    "#     No need for mergeSchema (default is faster)\n",
    "df = spark.read.parquet('/data/*.parquet')  # Faster!\n",
    "\n",
    "# 3. Very Large Number of Files\n",
    "#     10,000 parquet files\n",
    "#     mergeSchema will take forever!\n",
    "#     Better: Define schema explicitly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summary\n",
    "# mergeSchema = True means:\n",
    "\n",
    "# Read all parquet file schemas\n",
    "# Merge them into one unified schema\n",
    "# Handle missing columns automatically\n",
    "# Fill missing values with null\n",
    "\n",
    "# When to use:\n",
    "\n",
    "# Parquet files only\n",
    "# Schema evolution over time\n",
    "# Manageable number of files\n",
    "# Want simple solution\n",
    "\n",
    "# When NOT to use:\n",
    "\n",
    "# Many files (slow!)\n",
    "# Non-parquet formats\n",
    "# Known schema (define explicitly)\n",
    "# Performance critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5948e3a0-203e-418d-bce9-da1dd086868a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inconsistency in Pyspark , while processing data from multiple files with inconsistent schemas\n",
    "\n",
    "\n",
    "\n",
    "#what to use : \n",
    "    # 1. Always Use unionByName \n",
    "    # 2.  Define Master Schema\n",
    "        \"\"\"Have a reference schema\n",
    "        All data must conform to it\"\"\"\n",
    "        master_schema = {...}\n",
    "\n",
    "    # 3.  Add Metadata\n",
    "        # Track data lineage\n",
    "        df = df.withColumn(\"source_file\", lit(\"jan_sales.csv\")) \\\n",
    "       .withColumn(\"load_date\", current_timestamp())\n",
    "\n",
    "    # 4. Validate Early\n",
    "        # Check schema before processing\n",
    "        required_cols = [\"id\", \"product\", \"price\"]\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            raise ValueError(\"Invalid schema!\")\n",
    " \n",
    "    # 5. Log Schema Changes\n",
    "        # Log what you're doing\n",
    "        print(f\"Original columns: {df1.columns}\")\n",
    "        print(f\"Adding columns: {missing_cols}\")\n",
    "\n",
    "    ##  Key Takeaway: Use unionByName(allowMissingColumns=True) - it handles 90% of cases automatically!    print(f\"Final columns: {result.columns}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e336c6-811b-40b2-89b6-b195ca7de2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Q : what are the key diffrence between spark and haddop mapreduce , interms of flexiblity scaliblity : \n",
    "\n",
    "## Ans : \n",
    "\n",
    "# Spark vs Hadoop MapReduce - Key Differences\n",
    "\n",
    "## 1. Speed & Performance\n",
    "\n",
    "**Spark:**\n",
    "- In-memory processing\n",
    "- 100x faster for iterative algorithms\n",
    "- 10x faster for disk-based operations\n",
    "\n",
    "**MapReduce:**\n",
    "- Disk-based processing\n",
    "- Reads/writes to disk after each operation\n",
    "- Slower for iterative jobs\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ease of Use\n",
    "\n",
    "**Spark:**\n",
    "- High-level APIs (Python, Scala, Java, R, SQL)\n",
    "- Rich libraries (SQL, ML, Streaming, GraphX)\n",
    "- Interactive shell available\n",
    "- Less code (2-5x less than MapReduce)\n",
    "\n",
    "**MapReduce:**\n",
    "- Low-level API (only Java primarily)\n",
    "- Verbose code\n",
    "- No interactive mode\n",
    "- Limited built-in libraries\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Processing Model\n",
    "\n",
    "**Spark:**\n",
    "- Batch + Real-time streaming\n",
    "- Iterative processing (ML algorithms)\n",
    "- Interactive queries\n",
    "- DAG execution engine\n",
    "\n",
    "**MapReduce:**\n",
    "- Only batch processing\n",
    "- Two-step: Map → Reduce\n",
    "- Not suitable for iterative jobs\n",
    "- No interactive mode\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Flexibility\n",
    "\n",
    "**Spark:**\n",
    "- Multiple workloads: SQL, streaming, ML, graphs\n",
    "- Unified engine for everything\n",
    "- Can read from multiple sources\n",
    "\n",
    "**MapReduce:**\n",
    "- Only batch processing\n",
    "- Separate tools needed for streaming, SQL\n",
    "- Limited to Hadoop ecosystem\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Scalability\n",
    "\n",
    "**Spark:**\n",
    "- Scales horizontally\n",
    "- Better resource utilization\n",
    "- Dynamic resource allocation\n",
    "- Works on YARN, Mesos, K8s, Standalone\n",
    "\n",
    "**MapReduce:**\n",
    "- Scales horizontally\n",
    "- Only on YARN\n",
    "- Static resource allocation\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Fault Tolerance\n",
    "\n",
    "**Spark:**\n",
    "- RDD lineage (recompute lost partitions)\n",
    "- In-memory recovery\n",
    "- Faster recovery\n",
    "\n",
    "**MapReduce:**\n",
    "- Replication (data copied 3x)\n",
    "- Disk-based recovery\n",
    "- Slower recovery\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Real-Time Processing\n",
    "\n",
    "**Spark:**\n",
    "- ✅ Yes (Spark Streaming, Structured Streaming)\n",
    "- Micro-batch processing\n",
    "- Near real-time\n",
    "\n",
    "**MapReduce:**\n",
    "- ❌ No\n",
    "- Only batch processing\n",
    "- Not suitable for real-time\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Comparison Table\n",
    "\n",
    "| Feature | Spark | MapReduce |\n",
    "|---------|-------|-----------|\n",
    "| Speed | 100x faster | Baseline |\n",
    "| Memory | In-memory | Disk-based |\n",
    "| Ease of Use | Easy (Python, SQL) | Hard (Java) |\n",
    "| Real-time | Yes | No |\n",
    "| Iterative | Excellent | Poor |\n",
    "| Fault Tolerance | Lineage | Replication |\n",
    "| APIs | High-level | Low-level |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use What?\n",
    "\n",
    "**Use Spark:**\n",
    "- Machine Learning\n",
    "- Real-time analytics\n",
    "- Interactive queries\n",
    "- Iterative algorithms\n",
    "- Complex workflows\n",
    "\n",
    "**Use MapReduce:**\n",
    "- Simple batch jobs\n",
    "- Already invested in MapReduce\n",
    "- Stable, proven workloads\n",
    "- One-time transformations\n",
    "\n",
    "---\n",
    "\n",
    "## Interview Answer (Short):\n",
    "\n",
    "\"Spark is 100x faster than MapReduce because it processes data in-memory instead of disk. Spark supports real-time streaming, machine learning, and SQL with easy-to-use APIs in Python, Scala, and Java. MapReduce is disk-based, only does batch processing, and requires verbose Java code. Spark uses RDD lineage for fault tolerance while MapReduce uses data replication. For modern big data workloads, Spark is the clear choice due to its speed, flexibility, and unified platform for batch, streaming, ML, and SQL.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points to Remember:\n",
    "\n",
    "1. **Speed**: Spark in-memory vs MapReduce disk\n",
    "2. **Flexibility**: Spark does everything vs MapReduce only batch\n",
    "3. **Ease**: Spark Python/SQL vs MapReduce Java\n",
    "4. **Real-time**: Spark yes vs MapReduce no\n",
    "5. **Recovery**: Spark lineage vs MapReduce replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db121ef-184e-455d-b772-3fe93db7373f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. You are working with a real-time data pipeline, and you notice missing values in your streaming data Column - Category. How would you handle null or missing values in such a scenario?**\n",
    "\n",
    "**df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df2a0e4-a0b5-44b0-b6f1-675655dea923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Handling missing vlaue in your streaming data when working with columns - category , \n",
    "## HJandle Null or missign value in your streaming data\n",
    "df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")\n",
    "df_stream_cleaned = df_stream.fillna({\"Category\": \"N/A\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298715e0-35e5-470d-bbda-e99dd32e3add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0d8077-9082-4ad7-8a2c-2b0c15ae48d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user4\", 10), (\"user2\", 3)]\n",
    "columns = [\"user_id\", \"actions\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4d44c0-032e-442d-a75d-b1765b8f736b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.groupBy(\"user_id\")\\\n",
    "    .agg(sum(\"actions\").alias(\"total_actions\"))\\\n",
    "        .orderBy(\"total_actions\", ascending= False)\\\n",
    "            .limit(5))\n",
    "\n",
    "\n",
    "\n",
    "display(df)\n",
    "                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ec3c80-786b-4a80-a584-fcddc48c7284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5220aa-1f86-4b5d-ab3c-d885ddb562a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2023-12-01\", 100), (\"cust2\", \"2023-12-02\", 150),\n",
    "        (\"cust1\", \"2023-12-03\", 200), (\"cust2\", \"2023-12-04\", 250)]\n",
    "columns = [\"customer_id\", \"transaction_date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4629de25-0256-4d66-badb-318c861cd05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec33457-70c3-4a4e-b5e8-de430e9aab32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('transaction_date', col('transaction_date').cast(DateType()))\n",
    "\n",
    "df = df.withColumn('flag' , dense_rank().over(Window.partitionBy('customer_id').orderBy(col('transaction_date').desc()))).filter(col('flag') == 1)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f39777-5a20-4c03-95e5-3c998159e4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7. You need to identify customers who haven’t made any purchases in the last 30 days. How would you filter such customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a395399e-3b5a-431f-8b0d-93a3747d6313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2025-12-01\"), (\"cust2\", \"2024-11-20\"), (\"cust3\", \"2024-11-25\")]\n",
    "columns = [\"customer_id\", \"last_purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8435f3d2-59f2-4b65-9b3f-4ba28a18e7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# current_date() → today’s date (system date)\n",
    "\n",
    "# datediff(A, B) → number of days between A and B\n",
    "\n",
    "df = df.withColumn('last_purchase_date', to_date('last_purchase_date'))\n",
    "df = df.withColumn('gap' , datediff(current_date(), 'last_purchase_date')).filter(col('gap')> 30) \n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953a7ea9-4788-4465-b941-e5c2c60c4e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8. While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8961f80-1993-436d-9b86-9b98e5be98e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f10e86c-5352-42a8-8bdb-3df04a309e61",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770283057569}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Text into array using split fun \n",
    "# using explode , split \n",
    "df = df.withColumn('feedback' , lower('feedback')).withColumn('feedback' , explode(split('feedback',' ')))\n",
    "df_grp = df.groupBy(\"feedback\").agg(count(\"feedback\").alias('wordcount'))\n",
    "df_grp.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7252f72-5062-4ed9-95d1-a9f54dae4ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ea168e-bbf8-4e63-b325-bda69a19d9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff531350-5333-4556-94a1-2b15f5132b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#total sum , of sales , we'll using ag\n",
    "# cast date that is in string format into date format \n",
    "\n",
    "df = df.withColumn('date', to_date('date'))\n",
    "df = df.withColumn('CumSum' , sum('sales').over(Window.partitionBy('product_id').orderBy('date')) )\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afacc7c1-bece-4074-8d97-706ab567628b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting the original order?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa4abd1-a6ce-4590-ab94-88c35747fc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650e6662-e39f-4a56-b884-41510b9ed08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in this and previous one we apply row level transformation \n",
    "\n",
    "df = df.withColumn('rowflag', row_number().over(Window.partitionBy('name').orderBy('age'))).filter(col('rowflag') == 1)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9ed29e-cede-4329-ae1d-4e12ae13f085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c8f7b9-c8db-4b83-8df5-17d2d2d5998a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a4e320-3d70-4240-bcb5-1f18e71b2b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupBy('user_id').agg(avg(\"duration\").alias('avg_duration'))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7581ed7f-098c-456e-933e-3ff55d004088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1889a2-c7c8-4b52-ac44-0908e21125c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c784e77-e81f-4405-83a0-be1a272d2f31",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770289176217}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('date', to_date('date'))\n",
    "\n",
    "df = df.withColumn('date' , month('date')).groupBy(\"date\" , \"product_id\").agg(sum(\"sales\").alias('sales_sum'))\n",
    "df = df.withColumn('ranking' , dense_rank()\\\n",
    "    .over(Window.partitionBy('date')\\\n",
    "    .orderBy(col('sales_sum').desc())))\\\n",
    "    .filter(col('ranking') == 1)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77edac7c-7a65-41de-b393-9277f8ca7a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**what is role of SparkContext in pyspark** \n",
    "\n",
    "\n",
    "**13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and avoid data corruption in PySpark?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9beeb417-3cde-4676-bd29-60f227f54ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0862ca2-5f6d-4ce4-b6c3-339c5c74f5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "    .option('inferSchema', True)\\\n",
    "        .load('path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17f55cd-863f-428f-95e5-cee7cab5c0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41657147-3f1e-4443-b324-cf34baadde10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('mode', \"DROPMALFORMED\")\\\n",
    "        .load('stagging location')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6807f63-d276-4c42-9714-3aac9bd0fad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q16**\n",
    "    \n",
    "**A diffrence betwwen , RDDs , Dataframe , Dataset**\n",
    "\n",
    "**B What is Query Optimization , catalyst optimizer , join which one**\n",
    "\n",
    "**C Tell us about spark session**\n",
    "\n",
    "**D diff btw narrow and wide transformation**\n",
    "\n",
    "**E what is use of colease and repartition**\n",
    "\n",
    "**F when to use catch() and Persist()  whats diffrence**\n",
    "\n",
    "**G whats importance of partition in Pyspark**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319b8975-17e3-4da9-83af-41de79cb1412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Query Optimization in Spark\n",
    "\n",
    "## What is Query Optimization?\n",
    "\n",
    "Query optimization is the process where Spark transforms your query into the most efficient execution plan.\n",
    "\n",
    "---\n",
    "\n",
    "## The Flow\n",
    "```text\n",
    "User Query (SQL/DataFrame)\n",
    "        |\n",
    "        v\n",
    "    [Parser]\n",
    "        |\n",
    "        v\n",
    "Unresolved Logical Plan\n",
    "        |\n",
    "        v\n",
    "    [Analyzer]\n",
    "        |\n",
    "        v\n",
    "Resolved Logical Plan\n",
    "        |\n",
    "        v\n",
    "    [Optimizer]\n",
    "        |\n",
    "        v\n",
    "Optimized Logical Plan\n",
    "        |\n",
    "        v\n",
    "[Physical Planner]\n",
    "        |\n",
    "        v\n",
    "Multiple Physical Plans\n",
    "        |\n",
    "        v\n",
    "   [Cost Model]\n",
    "        |\n",
    "        v\n",
    "Best Physical Plan\n",
    "        |\n",
    "        v\n",
    "   [Execution]\n",
    "        |\n",
    "        v\n",
    "    Results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Two Main Components\n",
    "\n",
    "### 1. Logical Plan (What to do)\n",
    "- Describes WHAT operations to perform\n",
    "- Does not specify HOW to execute\n",
    "- Abstract representation of query\n",
    "- Multiple optimization rules applied\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "Filter (age > 30)\n",
    "    |\n",
    "Project (name, age)\n",
    "    |\n",
    "Scan (table)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Physical Plan (How to do it)\n",
    "- Describes HOW to execute operations\n",
    "- Specific execution strategy\n",
    "- Considers resources and data distribution\n",
    "- Multiple physical plans generated\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "HashAggregate\n",
    "    |\n",
    "Exchange (shuffle)\n",
    "    |\n",
    "FileScan (with pushed filters)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Difference\n",
    "\n",
    "| Aspect | Logical Plan | Physical Plan |\n",
    "|--------|--------------|---------------|\n",
    "| Focus | What to do | How to do it |\n",
    "| Level | Abstract | Concrete |\n",
    "| Examples | Filter, Join, Group | HashJoin, BroadcastJoin, SortMergeJoin |\n",
    "| Execution | No | Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "**Query:**\n",
    "```python\n",
    "df.filter(col(\"age\") > 30).select(\"name\", \"age\")\n",
    "```\n",
    "\n",
    "**Logical Plan:**\n",
    "```text\n",
    "What: Filter by age, then select columns\n",
    "```\n",
    "\n",
    "**Physical Plan:**\n",
    "```text\n",
    "How: \n",
    "- FileScan with pushed filter (age > 30)\n",
    "- Project columns (name, age)\n",
    "- Use 8 partitions\n",
    "- BroadcastHashJoin if needed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Point\n",
    "\n",
    "Query optimization converts your high-level query (Logical Plan) into the fastest execution strategy (Physical Plan) automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d124973a-e657-4291-a127-d3ed3ed8af4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**22. You have a dataset containing the names of employees and their departments. You need to find the department with the most employees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4cc39f3-3f62-4016-9854-4a6269c3c6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Alice\", \"HR\"), (\"Bob\", \"Finance\"), (\"Charlie\", \"HR\"), (\"David\", \"Engineering\"), (\"Eve\", \"Finance\")]\n",
    "columns = [\"employee_name\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec9b713-9234-435d-b6fb-6bead8d6d9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupBy(\"department\").agg(count(\"*\").alias(\"total_employee\")).sort('total_employee' , ascending=False)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a078f9bd-7dad-4501-ba7a-b4668b2dcd62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**23. While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you achieve this using a when condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bdb37e-5c8d-4c31-a013-98167c717768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce482832-3727-4b7b-9158-3f54cde4ff42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('price_cat', when(col('sales') > 50 , \"High\").otherwise(\"Low\"))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060e9c3e-109e-4dea-84af-cc66b0407ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**24. While analyzing a large dataset, you need to create a new column that holds a timestamp of when the record was processed. How would you implement this and what can be the best USE CASE?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61de48c5-8f56-4cb3-927d-cde55bd44e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461467b2-10f5-4c45-a9b2-48867899ca74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"processed_time\" , current_timestamp())\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1b94dd-cedf-4a4f-8b0d-d6e488e3c7e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**25. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it. How would you achieve this?**\n",
    "\n",
    "\n",
    "### if someoene dont want to use python queri , thne change its view , bez they wanna use sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b0e278-d64c-473d-84d4-a872d58f2b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448009b9-68d1-43f5-8fd1-f1700785f626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temsqldf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e5f2ae-a186-455c-8900-7db0c7e9ffe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temsqldf\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Interview-Pyspark-Q",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

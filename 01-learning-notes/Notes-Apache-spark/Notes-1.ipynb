{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2eab68-8504-46ee-9e60-6a9dd0b086fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Apache Spark Architecture - Complete Overview\n",
    "\n",
    "## What is Apache Spark?\n",
    "\n",
    "**Apache Spark** is a unified analytics engine for large-scale data processing.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Fast:** In-memory processing (100x faster than MapReduce)\n",
    "- **Distributed:** Processes data across multiple machines\n",
    "- **General-purpose:** Batch, streaming, ML, SQL, graph processing\n",
    "- **Fault-tolerant:** Recovers from failures automatically\n",
    "\n",
    "### Why Spark?\n",
    "- Process Big Data (TB to PB scale)\n",
    "- Parallel processing\n",
    "- In-memory computation\n",
    "- Rich APIs (Python, Scala, Java, R, SQL)\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Spark Architecture\n",
    "\n",
    "```text\n",
    "Spark Architecture Overview\n",
    "============================\n",
    "\n",
    "           Driver Program\n",
    "    (Your Application Main)\n",
    "              |\n",
    "              |\n",
    "    +---------+---------+\n",
    "    |                   |\n",
    "    v                   v\n",
    "Cluster Manager    SparkContext\n",
    "(YARN/Mesos/K8s)   (Coordination)\n",
    "    |                   |\n",
    "    +-------+-----------+\n",
    "            |\n",
    "    +-------+-------+-------+\n",
    "    |       |       |       |\n",
    "    v       v       v       v\n",
    "Worker   Worker  Worker  Worker\n",
    "Node 1   Node 2  Node 3  Node 4\n",
    "  |        |       |       |\n",
    "Executor Executor Executor Executor\n",
    "(Tasks)  (Tasks)  (Tasks)  (Tasks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. Driver Program\n",
    "**What it is:**\n",
    "- Your application's main program\n",
    "- Contains SparkContext\n",
    "- Runs the main() function\n",
    "\n",
    "**Responsibilities:**\n",
    "- Analyzes user code\n",
    "- Creates execution plan (DAG)\n",
    "- Schedules tasks\n",
    "- Monitors execution\n",
    "- Collects results\n",
    "\n",
    "**Location:**\n",
    "- Can run on client machine\n",
    "- Or on cluster (cluster mode)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# This code runs on Driver\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "df = spark.read.parquet(\"/data\")\n",
    "result = df.count()  # Driver coordinates this\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. SparkContext\n",
    "\n",
    "**What it is:**\n",
    "- Entry point to Spark functionality\n",
    "- Connection to Spark cluster\n",
    "- Coordinates execution\n",
    "\n",
    "**Key Functions:**\n",
    "- Creates RDDs\n",
    "- Broadcasts variables\n",
    "- Manages configuration\n",
    "- Monitors application\n",
    "\n",
    "**In modern Spark:**\n",
    "```python\n",
    "# SparkSession wraps SparkContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext  # Access SparkContext\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Cluster Manager\n",
    "\n",
    "**What it is:**\n",
    "- External service that manages cluster resources\n",
    "- Allocates resources to applications\n",
    "\n",
    "**Types:**\n",
    "1. **Standalone:** Spark's built-in manager\n",
    "2. **YARN:** Hadoop's resource manager\n",
    "3. **Mesos:** Apache Mesos\n",
    "4. **Kubernetes:** Container orchestration\n",
    "\n",
    "**Role:**\n",
    "- Accept application requests\n",
    "- Allocate executors to workers\n",
    "- Monitor resource usage\n",
    "- Handle failures\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Worker Nodes\n",
    "\n",
    "**What they are:**\n",
    "- Physical/virtual machines in cluster\n",
    "- Host executor processes\n",
    "\n",
    "**Responsibilities:**\n",
    "- Run executors\n",
    "- Manage local resources (CPU, memory, disk)\n",
    "- Report status to cluster manager\n",
    "\n",
    "**Example Cluster:**\n",
    "```text\n",
    "Worker Node 1 (32GB RAM, 8 cores)\n",
    "    - Executor 1 (8GB, 2 cores)\n",
    "    - Executor 2 (8GB, 2 cores)\n",
    "\n",
    "Worker Node 2 (32GB RAM, 8 cores)\n",
    "    - Executor 3 (8GB, 2 cores)\n",
    "    - Executor 4 (8GB, 2 cores)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Executors\n",
    "\n",
    "**What they are:**\n",
    "- JVM processes running on worker nodes\n",
    "- Execute tasks\n",
    "- Store data in memory/disk\n",
    "\n",
    "**Responsibilities:**\n",
    "- Run tasks assigned by driver\n",
    "- Store computed data\n",
    "- Return results to driver\n",
    "- Cache data when requested\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Architecture Diagram\n",
    "\n",
    "```text\n",
    "Detailed Spark Architecture\n",
    "============================\n",
    "\n",
    "Client Machine / Driver Node\n",
    "+----------------------------------+\n",
    "|  Driver Program                  |\n",
    "|  +--------------------------+    |\n",
    "|  | SparkContext             |    |\n",
    "|  | - DAG Scheduler          |    |\n",
    "|  | - Task Scheduler         |    |\n",
    "|  | - Backend Scheduler      |    |\n",
    "|  +--------------------------+    |\n",
    "+----------------------------------+\n",
    "            |\n",
    "            | Submit Application\n",
    "            v\n",
    "+----------------------------------+\n",
    "|  Cluster Manager                 |\n",
    "|  (YARN / Mesos / K8s)           |\n",
    "|  - Resource Allocation           |\n",
    "|  - Executor Launch               |\n",
    "+----------------------------------+\n",
    "            |\n",
    "            | Launch Executors\n",
    "            v\n",
    "+----------------------------------+\n",
    "|  Worker Nodes (Cluster)          |\n",
    "|                                  |\n",
    "|  Worker Node 1                   |\n",
    "|  +----------------------------+  |\n",
    "|  | Executor 1                 |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | | Task 1 | Task 2       | |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | Cache / Storage           |  |\n",
    "|  +----------------------------+  |\n",
    "|                                  |\n",
    "|  Worker Node 2                   |\n",
    "|  +----------------------------+  |\n",
    "|  | Executor 2                 |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | | Task 3 | Task 4       | |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | Cache / Storage           |  |\n",
    "|  +----------------------------+  |\n",
    "|                                  |\n",
    "|  Worker Node 3                   |\n",
    "|  +----------------------------+  |\n",
    "|  | Executor 3                 |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | | Task 5 | Task 6       | |  |\n",
    "|  | +------------------------+ |  |\n",
    "|  | Cache / Storage           |  |\n",
    "|  +----------------------------+  |\n",
    "+----------------------------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Job Execution Flow\n",
    "\n",
    "### Step-by-Step Execution\n",
    "\n",
    "```text\n",
    "Step 1: User Submits Application\n",
    "=================================\n",
    "spark-submit app.py\n",
    "    |\n",
    "    v\n",
    "Driver Program Starts\n",
    "\n",
    "Step 2: Driver Creates SparkContext\n",
    "====================================\n",
    "SparkContext initializes\n",
    "    |\n",
    "    v\n",
    "Connects to Cluster Manager\n",
    "\n",
    "Step 3: Resource Allocation\n",
    "============================\n",
    "Cluster Manager allocates resources\n",
    "    |\n",
    "    v\n",
    "Launches Executors on Worker Nodes\n",
    "\n",
    "Step 4: Code Execution\n",
    "======================\n",
    "Driver sends application code to Executors\n",
    "    |\n",
    "    v\n",
    "Driver creates DAG from transformations\n",
    "\n",
    "Step 5: Task Scheduling\n",
    "========================\n",
    "Driver converts DAG to stages\n",
    "    |\n",
    "    v\n",
    "Driver creates tasks (one per partition)\n",
    "    |\n",
    "    v\n",
    "Driver sends tasks to Executors\n",
    "\n",
    "Step 6: Task Execution\n",
    "======================\n",
    "Executors run tasks in parallel\n",
    "    |\n",
    "    v\n",
    "Process data partitions\n",
    "\n",
    "Step 7: Results Collection\n",
    "===========================\n",
    "Executors send results back to Driver\n",
    "    |\n",
    "    v\n",
    "Driver aggregates final results\n",
    "\n",
    "Step 8: Completion\n",
    "==================\n",
    "Application completes\n",
    "    |\n",
    "    v\n",
    "Resources released\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Execution Example\n",
    "\n",
    "```python\n",
    "# User code\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.filter(col(\"age\") > 30)\n",
    "df = df.groupBy(\"city\").count()\n",
    "result = df.show()\n",
    "```\n",
    "\n",
    "### What Happens Internally:\n",
    "\n",
    "```text\n",
    "1. Driver: Read code, build logical plan\n",
    "   Logical Plan:\n",
    "   - Read /data\n",
    "   - Filter age > 30\n",
    "   - GroupBy city\n",
    "   - Count\n",
    "   - Show\n",
    "\n",
    "2. Driver: Optimize plan (Catalyst Optimizer)\n",
    "   Optimized Plan:\n",
    "   - Read only needed columns\n",
    "   - Push filter to source\n",
    "   - Optimize groupBy\n",
    "\n",
    "3. Driver: Create physical plan\n",
    "   Physical Plan:\n",
    "   - Stage 1: Read + Filter (narrow)\n",
    "   - Stage 2: GroupBy + Count (wide, shuffle)\n",
    "\n",
    "4. Driver: Create tasks\n",
    "   Stage 1: 8 tasks (8 partitions)\n",
    "   Stage 2: 4 tasks (4 output partitions)\n",
    "\n",
    "5. Cluster Manager: Allocate resources\n",
    "   Launch 3 executors across worker nodes\n",
    "\n",
    "6. Driver: Send Stage 1 tasks to Executors\n",
    "   Executor 1: Tasks 1, 2, 3\n",
    "   Executor 2: Tasks 4, 5, 6\n",
    "   Executor 3: Tasks 7, 8\n",
    "\n",
    "7. Executors: Execute Stage 1\n",
    "   Read data, apply filter, write shuffle files\n",
    "\n",
    "8. Driver: Send Stage 2 tasks to Executors\n",
    "   Executor 1: Task 1\n",
    "   Executor 2: Task 2\n",
    "   Executor 3: Task 3, 4\n",
    "\n",
    "9. Executors: Execute Stage 2\n",
    "   Read shuffle data, groupBy, count\n",
    "\n",
    "10. Executors: Return results to Driver\n",
    "    Driver collects and displays results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Driver Node vs Worker Node\n",
    "\n",
    "### Driver Node\n",
    "\n",
    "**Role:** Coordinator and planner\n",
    "\n",
    "**Contains:**\n",
    "- Driver Program\n",
    "- SparkContext\n",
    "- DAG Scheduler\n",
    "- Task Scheduler\n",
    "\n",
    "**Responsibilities:**\n",
    "- Parse user code\n",
    "- Create execution plan\n",
    "- Schedule tasks\n",
    "- Monitor progress\n",
    "- Collect results\n",
    "\n",
    "**Resources Needed:**\n",
    "- Moderate CPU (for planning)\n",
    "- Moderate memory (for coordination)\n",
    "- Network bandwidth (for task distribution)\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "spark-submit \\\n",
    "  --driver-memory 4g \\\n",
    "  --driver-cores 2 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Worker Node\n",
    "\n",
    "**Role:** Executor and processor\n",
    "\n",
    "**Contains:**\n",
    "- One or more Executors\n",
    "- Task execution engine\n",
    "- Data cache/storage\n",
    "\n",
    "**Responsibilities:**\n",
    "- Execute tasks\n",
    "- Process data\n",
    "- Store intermediate results\n",
    "- Return results to driver\n",
    "\n",
    "**Resources Needed:**\n",
    "- High CPU (for processing)\n",
    "- High memory (for data + cache)\n",
    "- Disk space (for shuffle files)\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "spark-submit \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 10 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Communication Flow\n",
    "\n",
    "```text\n",
    "Driver <---> Cluster Manager\n",
    "    |\n",
    "    +---> Executor 1 (Worker Node 1)\n",
    "    |         |\n",
    "    |         +---> Task execution\n",
    "    |         +---> Send results\n",
    "    |\n",
    "    +---> Executor 2 (Worker Node 2)\n",
    "    |         |\n",
    "    |         +---> Task execution\n",
    "    |         +---> Send results\n",
    "    |\n",
    "    +---> Executor 3 (Worker Node 3)\n",
    "              |\n",
    "              +---> Task execution\n",
    "              +---> Send results\n",
    "\n",
    "Executors <---> Executors (during shuffle)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Job Execution Hierarchy\n",
    "\n",
    "```text\n",
    "Application\n",
    "    |\n",
    "    +---> Job 1 (triggered by action 1)\n",
    "    |       |\n",
    "    |       +---> Stage 1 (narrow transformations)\n",
    "    |       |       |\n",
    "    |       |       +---> Task 1 (partition 1)\n",
    "    |       |       +---> Task 2 (partition 2)\n",
    "    |       |       +---> Task 3 (partition 3)\n",
    "    |       |\n",
    "    |       +---> Stage 2 (wide transformation, shuffle)\n",
    "    |               |\n",
    "    |               +---> Task 1 (partition 1)\n",
    "    |               +---> Task 2 (partition 2)\n",
    "    |\n",
    "    +---> Job 2 (triggered by action 2)\n",
    "            |\n",
    "            +---> Stage 1\n",
    "                    |\n",
    "                    +---> Tasks...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Memory Architecture\n",
    "\n",
    "### Driver Memory\n",
    "```text\n",
    "Driver Memory (4GB example)\n",
    "===========================\n",
    "+---------------------------+\n",
    "| User Code Memory          | 60%\n",
    "| (Application variables)   |\n",
    "+---------------------------+\n",
    "| Spark Internal Memory     | 30%\n",
    "| (Task scheduling, DAG)    |\n",
    "+---------------------------+\n",
    "| Reserved Memory           | 10%\n",
    "+---------------------------+\n",
    "```\n",
    "\n",
    "### Executor Memory\n",
    "```text\n",
    "Executor Memory (8GB example)\n",
    "==============================\n",
    "+---------------------------+\n",
    "| Execution Memory          | 40%\n",
    "| (Task computation)        |\n",
    "+---------------------------+\n",
    "| Storage Memory            | 40%\n",
    "| (Cached data, broadcast)  |\n",
    "+---------------------------+\n",
    "| User Memory               | 15%\n",
    "| (User data structures)    |\n",
    "+---------------------------+\n",
    "| Reserved Memory           | 5%\n",
    "+---------------------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Modes\n",
    "\n",
    "### 1. Client Mode\n",
    "```text\n",
    "Client Machine (Driver)\n",
    "    |\n",
    "    | Task distribution\n",
    "    v\n",
    "Cluster (Executors only)\n",
    "```\n",
    "- Driver runs on client machine\n",
    "- Good for interactive analysis\n",
    "- Requires client to stay connected\n",
    "\n",
    "### 2. Cluster Mode\n",
    "```text\n",
    "Cluster\n",
    "    |\n",
    "    +---> Driver (on cluster)\n",
    "    |\n",
    "    +---> Executors (on cluster)\n",
    "```\n",
    "- Driver runs on cluster\n",
    "- Good for production jobs\n",
    "- Client can disconnect\n",
    "\n",
    "---\n",
    "\n",
    "## Production Architecture Example\n",
    "\n",
    "```text\n",
    "Production Spark Cluster\n",
    "=========================\n",
    "\n",
    "Driver Node (16GB RAM, 4 cores)\n",
    "    - Runs Spark Driver\n",
    "    - Monitors application\n",
    "\n",
    "Worker Node 1 (64GB RAM, 16 cores)\n",
    "    - Executor 1 (16GB, 4 cores)\n",
    "    - Executor 2 (16GB, 4 cores)\n",
    "    - Executor 3 (16GB, 4 cores)\n",
    "\n",
    "Worker Node 2 (64GB RAM, 16 cores)\n",
    "    - Executor 4 (16GB, 4 cores)\n",
    "    - Executor 5 (16GB, 4 cores)\n",
    "    - Executor 6 (16GB, 4 cores)\n",
    "\n",
    "Worker Node 3 (64GB RAM, 16 cores)\n",
    "    - Executor 7 (16GB, 4 cores)\n",
    "    - Executor 8 (16GB, 4 cores)\n",
    "    - Executor 9 (16GB, 4 cores)\n",
    "\n",
    "Total Resources:\n",
    "- 1 Driver\n",
    "- 9 Executors\n",
    "- 144 GB executor memory\n",
    "- 36 executor cores\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "| Component | Role | Location |\n",
    "|-----------|------|----------|\n",
    "| Driver | Coordinator | Driver Node |\n",
    "| SparkContext | Entry point | Driver Node |\n",
    "| Cluster Manager | Resource manager | Separate service |\n",
    "| Worker Node | Host for executors | Cluster |\n",
    "| Executor | Task executor | Worker Node |\n",
    "| Task | Unit of work | Executor |\n",
    "\n",
    "---\n",
    "\n",
    "## Resource Allocation Strategy\n",
    "\n",
    "### Small Job (< 100GB data)\n",
    "```python\n",
    "spark-submit \\\n",
    "  --driver-memory 2g \\\n",
    "  --executor-memory 4g \\\n",
    "  --executor-cores 2 \\\n",
    "  --num-executors 5 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "### Medium Job (100GB - 1TB data)\n",
    "```python\n",
    "spark-submit \\\n",
    "  --driver-memory 4g \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 20 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "### Large Job (> 1TB data)\n",
    "```python\n",
    "spark-submit \\\n",
    "  --driver-memory 8g \\\n",
    "  --executor-memory 16g \\\n",
    "  --executor-cores 5 \\\n",
    "  --num-executors 50 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "### 1. What is Apache Spark\n",
    "- Distributed analytics engine\n",
    "- In-memory processing\n",
    "- General-purpose framework\n",
    "\n",
    "### 2. High-Level Architecture\n",
    "- Driver coordinates\n",
    "- Cluster Manager allocates\n",
    "- Executors process\n",
    "- Workers host executors\n",
    "\n",
    "### 3. Driver Node and Worker Node\n",
    "- Driver: Planning and coordination\n",
    "- Worker: Execution and processing\n",
    "\n",
    "### 4. Job Execution Flow\n",
    "- Code submission\n",
    "- Resource allocation\n",
    "- Task creation\n",
    "- Parallel execution\n",
    "- Result collection\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "```text\n",
    "Complete Flow Recap\n",
    "===================\n",
    "\n",
    "User Code\n",
    "    |\n",
    "    v\n",
    "Driver (SparkContext)\n",
    "    |\n",
    "    +---> Builds DAG\n",
    "    +---> Optimizes Plan\n",
    "    +---> Creates Jobs\n",
    "    +---> Divides into Stages\n",
    "    +---> Generates Tasks\n",
    "    |\n",
    "    v\n",
    "Cluster Manager\n",
    "    |\n",
    "    +---> Allocates Resources\n",
    "    +---> Launches Executors\n",
    "    |\n",
    "    v\n",
    "Worker Nodes\n",
    "    |\n",
    "    +---> Executor 1 ---> Tasks\n",
    "    +---> Executor 2 ---> Tasks\n",
    "    +---> Executor 3 ---> Tasks\n",
    "    |\n",
    "    v\n",
    "Results back to Driver\n",
    "    |\n",
    "    v\n",
    "User gets output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the architecture, you can learn:\n",
    "- Transformations and Actions\n",
    "- Partitioning strategies\n",
    "- Performance tuning\n",
    "- Memory management\n",
    "- Shuffle optimization\n",
    "- Caching strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "**Remember the hierarchy:**\n",
    "```text\n",
    "Application > Job > Stage > Task\n",
    "\n",
    "Application = Your entire program\n",
    "Job = One action\n",
    "Stage = Set of transformations (divided by shuffle)\n",
    "Task = Processing one partition\n",
    "```\n",
    "\n",
    "**Remember the components:**\n",
    "```text\n",
    "Driver = Brain (plans)\n",
    "Cluster Manager = Resource allocator\n",
    "Worker = Physical machine\n",
    "Executor = Worker process\n",
    "Task = Smallest unit of work\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:** Spark distributes your code across a cluster, the Driver coordinates, and Executors do the actual work in parallel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03801741-5d35-4714-b22a-a94dd991f8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Application Master Container â€“ PySpark to JVM Flow\n",
    "\n",
    "## Overview\n",
    "\n",
    "Understanding how PySpark code executes is crucial for debugging and optimization in production environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "```text\n",
    "Application Master Container (Driver Node)\n",
    "============================================\n",
    "\n",
    "   User PySpark Code\n",
    "   (df.count(), df.show())\n",
    "           |\n",
    "           v\n",
    "   PySpark Main (PySpark driver)\n",
    "   (Python Process)\n",
    "           |\n",
    "           |  Py4J (Python <-> JVM bridge)\n",
    "           v\n",
    "   JVM Main (Application Driver)\n",
    "   (Spark Driver - Java/Scala)\n",
    "           |\n",
    "           v\n",
    "   Spark Core Engine\n",
    "   (DAG, Jobs, Stages, Tasks)\n",
    "           |\n",
    "           v\n",
    "   Cluster Manager\n",
    "   (YARN/Mesos/Kubernetes)\n",
    "           |\n",
    "           v\n",
    "   Executors (Worker Nodes)\n",
    "   (Task Execution)\n",
    "\n",
    "============================================\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Execution Flow\n",
    "\n",
    "### Step 1: User Writes PySpark Code\n",
    "```python\n",
    "# User writes code in Python\n",
    "df = spark.read.parquet(\"/data\")\n",
    "result = df.filter(df.age > 30).count()\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Code is written in Python\n",
    "- Runs in PySpark environment\n",
    "- Uses DataFrame API\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: PySpark Main Process\n",
    "```text\n",
    "PySpark Main (Python Process)\n",
    "    |\n",
    "    +-> Interprets Python code\n",
    "    +-> Validates DataFrame operations\n",
    "    +-> Prepares instructions for JVM\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- PySpark Main is a Python process\n",
    "- Python itself CANNOT execute Spark logic\n",
    "- Acts as a client/wrapper\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Py4J Bridge\n",
    "```text\n",
    "Python Side              Py4J Bridge              JVM Side\n",
    "    |                         |                      |\n",
    "    |  df.count()            |                      |\n",
    "    +----------------------->|                      |\n",
    "    |                         |  JavaObject.count() |\n",
    "    |                         +-------------------->|\n",
    "    |                         |                      |\n",
    "    |                         |  <-- Result (100) --|\n",
    "    |  <-- Return 100 --------|                      |\n",
    "```\n",
    "\n",
    "**What is Py4J?**\n",
    "- A library that enables Python to call JVM methods\n",
    "- Converts Python objects to Java objects\n",
    "- Serializes data between Python and JVM\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Python code\n",
    "df.count()\n",
    "\n",
    "# Py4J translates to\n",
    "# JavaObject.DataFrame.count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: JVM Main (Spark Driver)\n",
    "```text\n",
    "JVM Main (Spark Driver)\n",
    "    |\n",
    "    +-> Receives instruction from Py4J\n",
    "    +-> Executes Spark Core logic\n",
    "    +-> Creates execution plan\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- This is the REAL Spark Driver\n",
    "- Written in Scala/Java\n",
    "- Contains all Spark optimization logic\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Spark Core Engine\n",
    "```text\n",
    "Spark Core Engine\n",
    "    |\n",
    "    +-> Step 1: Build DAG (Directed Acyclic Graph)\n",
    "    |\n",
    "    +-> Step 2: Create Jobs (from Actions)\n",
    "    |\n",
    "    +-> Step 3: Divide into Stages (based on shuffles)\n",
    "    |\n",
    "    +-> Step 4: Break into Tasks (one per partition)\n",
    "    |\n",
    "    +-> Step 5: Schedule Tasks on Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Task Execution\n",
    "```text\n",
    "Cluster Manager (YARN/Mesos/K8s)\n",
    "    |\n",
    "    +-> Executor 1 (Worker Node 1)\n",
    "    |       |\n",
    "    |       +-> Task 1 (Partition 1)\n",
    "    |       +-> Task 2 (Partition 2)\n",
    "    |\n",
    "    +-> Executor 2 (Worker Node 2)\n",
    "    |       |\n",
    "    |       +-> Task 3 (Partition 3)\n",
    "    |       +-> Task 4 (Partition 4)\n",
    "    |\n",
    "    +-> Executor 3 (Worker Node 3)\n",
    "            |\n",
    "            +-> Task 5 (Partition 5)\n",
    "            +-> Task 6 (Partition 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Flow Example\n",
    "\n",
    "```python\n",
    "# User code\n",
    "df = spark.read.parquet(\"/data\")\n",
    "result = df.filter(df.age > 30).count()\n",
    "```\n",
    "\n",
    "### Execution Steps:\n",
    "\n",
    "**1. Python Process (PySpark Main):**\n",
    "```text\n",
    "User Code -> PySpark API\n",
    "```\n",
    "\n",
    "**2. Py4J Bridge:**\n",
    "```text\n",
    "Python df.count() -> Py4J -> Java DataFrame.count()\n",
    "```\n",
    "\n",
    "**3. JVM Spark Driver:**\n",
    "```text\n",
    "Receives count() instruction\n",
    "Creates DAG\n",
    "Identifies Action (count)\n",
    "Creates 1 Job\n",
    "```\n",
    "\n",
    "**4. Spark Core:**\n",
    "```text\n",
    "Job 1:\n",
    "  Stage 1: Read + Filter (narrow)\n",
    "  Stage 2: Count aggregation (wide)\n",
    "```\n",
    "\n",
    "**5. Task Creation:**\n",
    "```text\n",
    "Stage 1: 8 tasks (8 partitions)\n",
    "Stage 2: 1 task (final aggregation)\n",
    "```\n",
    "\n",
    "**6. Execution:**\n",
    "```text\n",
    "Tasks sent to Executors\n",
    "Results collected back to Driver\n",
    "Py4J returns result to Python\n",
    "User gets: result = 1500\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Component Breakdown\n",
    "\n",
    "### PySpark Main (Python Side)\n",
    "**Role:** Client-side wrapper\n",
    "\n",
    "**Responsibilities:**\n",
    "- Parse Python code\n",
    "- Validate DataFrame operations\n",
    "- Manage Python-side objects\n",
    "- Handle serialization\n",
    "\n",
    "**Limitations:**\n",
    "- Cannot execute Spark logic\n",
    "- Depends on JVM for actual processing\n",
    "\n",
    "---\n",
    "\n",
    "### Py4J Bridge\n",
    "**Role:** Communication layer\n",
    "\n",
    "**How it works:**\n",
    "```text\n",
    "Python Object  <--Py4J-->  Java Object\n",
    "   |                           |\n",
    "   +-- Serialization -->       |\n",
    "   |                           |\n",
    "   |       <-- Deserialization-+\n",
    "```\n",
    "\n",
    "**What gets transferred:**\n",
    "- DataFrame operations (lazy)\n",
    "- Action triggers\n",
    "- Configuration parameters\n",
    "- Results (after execution)\n",
    "\n",
    "---\n",
    "\n",
    "### JVM Main (Spark Driver)\n",
    "**Role:** Execution orchestrator\n",
    "\n",
    "**Components:**\n",
    "- SparkContext\n",
    "- DAGScheduler\n",
    "- TaskScheduler\n",
    "- Catalyst Optimizer (for DataFrames)\n",
    "\n",
    "**Key Functions:**\n",
    "- Query optimization\n",
    "- Execution planning\n",
    "- Resource management\n",
    "- Task distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Architecture?\n",
    "\n",
    "### Advantages:\n",
    "1. **Python ease of use** with **JVM performance**\n",
    "2. Spark optimizations work regardless of language\n",
    "3. Python developers can use Spark without learning Scala/Java\n",
    "4. Catalyst optimizer works the same for all APIs\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Serialization overhead** (Python <-> JVM)\n",
    "2. **Extra process** (Python process)\n",
    "3. **Memory duplication** (data in both Python and JVM)\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Implications\n",
    "\n",
    "### Where Performance is Lost:\n",
    "\n",
    "**1. Py4J Communication:**\n",
    "```python\n",
    "# BAD: Frequent small operations\n",
    "for i in range(1000):\n",
    "    df = df.withColumn(f\"col_{i}\", lit(i))  # 1000 Py4J calls!\n",
    "\n",
    "# GOOD: Single operation\n",
    "from pyspark.sql.functions import *\n",
    "df = df.select(\"*\", *[lit(i).alias(f\"col_{i}\") for i in range(1000)])\n",
    "```\n",
    "\n",
    "**2. UDF (User Defined Functions):**\n",
    "```python\n",
    "# BAD: Python UDF (slow, serialization overhead)\n",
    "from pyspark.sql.functions import udf\n",
    "@udf(\"int\")\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "df = df.withColumn(\"new_col\", add_one(col(\"age\")))\n",
    "\n",
    "# GOOD: Built-in functions (JVM, fast)\n",
    "df = df.withColumn(\"new_col\", col(\"age\") + 1)\n",
    "```\n",
    "\n",
    "**3. collect() Operations:**\n",
    "```python\n",
    "# BAD: Bringing data to Python\n",
    "data = df.collect()  # Serializes ALL data to Python\n",
    "for row in data:\n",
    "    print(row.age)\n",
    "\n",
    "# GOOD: Process in Spark\n",
    "df.select(\"age\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "### 1. Minimize Python UDFs\n",
    "```python\n",
    "# AVOID Python UDFs\n",
    "@udf(\"string\")\n",
    "def format_name(name):\n",
    "    return name.upper()\n",
    "\n",
    "# USE Built-in functions\n",
    "from pyspark.sql.functions import upper\n",
    "df = df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
    "```\n",
    "\n",
    "### 2. Use Pandas UDFs for Better Performance\n",
    "```python\n",
    "# Pandas UDF (vectorized, faster than regular UDF)\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def multiply_by_two(series: pd.Series) -> pd.Series:\n",
    "    return series * 2\n",
    "\n",
    "df = df.withColumn(\"doubled\", multiply_by_two(col(\"value\")))\n",
    "```\n",
    "\n",
    "### 3. Avoid collect() on Large Data\n",
    "```python\n",
    "# BAD\n",
    "all_data = df.collect()  # Brings all data to driver\n",
    "\n",
    "# GOOD\n",
    "df.write.parquet(\"/output\")  # Process in distributed manner\n",
    "```\n",
    "\n",
    "### 4. Use Arrow for Pandas Conversion\n",
    "```python\n",
    "# Enable Arrow optimization\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Faster conversion\n",
    "pandas_df = spark_df.toPandas()\n",
    "```\n",
    "\n",
    "### 5. Batch Operations\n",
    "```python\n",
    "# BAD: Multiple API calls\n",
    "df = df.withColumn(\"col1\", lit(1))\n",
    "df = df.withColumn(\"col2\", lit(2))\n",
    "df = df.withColumn(\"col3\", lit(3))\n",
    "\n",
    "# GOOD: Single call\n",
    "df = df.select(\"*\", lit(1).alias(\"col1\"), \n",
    "                    lit(2).alias(\"col2\"), \n",
    "                    lit(3).alias(\"col3\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Debugging Tips\n",
    "\n",
    "### 1. Check Py4J Connection\n",
    "```python\n",
    "# Verify Py4J is working\n",
    "print(spark._jvm)  # Should show JVM gateway object\n",
    "```\n",
    "\n",
    "### 2. Monitor Serialization\n",
    "```python\n",
    "# Check serialization time in Spark UI\n",
    "# Go to Stage Details -> Task Metrics -> Serialization Time\n",
    "```\n",
    "\n",
    "### 3. Profile Python Code\n",
    "```python\n",
    "import cProfile\n",
    "cProfile.run('df.count()')\n",
    "```\n",
    "\n",
    "### 4. Check Driver Logs\n",
    "```bash\n",
    "# Driver logs show Py4J communication\n",
    "# Look for: py4j.java_gateway logs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### Issue 1: Py4J Gateway Timeout\n",
    "**Symptom:** `Py4JNetworkError: An error occurred while trying to connect`\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Increase timeout\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "            \"-Dpy4j.gateway.startup.timeout=60\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Issue 2: Serialization Errors\n",
    "**Symptom:** `PicklingError` or `SerializationError`\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Use built-in functions instead of UDFs\n",
    "# Or ensure objects are serializable\n",
    "```\n",
    "\n",
    "### Issue 3: Slow Performance\n",
    "**Symptom:** Job takes much longer than expected\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# 1. Avoid Python UDFs\n",
    "# 2. Use Pandas UDFs if needed\n",
    "# 3. Minimize collect() calls\n",
    "# 4. Enable Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "### PySpark vs Scala Spark\n",
    "\n",
    "```text\n",
    "PySpark:\n",
    "Python Code -> Py4J -> JVM -> Spark Core -> Executors\n",
    "\n",
    "Scala Spark:\n",
    "Scala Code -> JVM -> Spark Core -> Executors\n",
    "\n",
    "Key Difference: Extra Py4J layer in PySpark\n",
    "```\n",
    "\n",
    "**Performance Impact:**\n",
    "- DataFrame operations: Similar (optimized by Catalyst)\n",
    "- RDD operations: Scala is faster\n",
    "- UDFs: Scala is much faster\n",
    "- Complex transformations: Scala is faster\n",
    "\n",
    "---\n",
    "\n",
    "## Memory Layout\n",
    "\n",
    "```text\n",
    "Driver Node Memory Layout\n",
    "==========================\n",
    "\n",
    "[Python Process Memory]\n",
    "    - PySpark objects\n",
    "    - Python variables\n",
    "    - Py4J client\n",
    "        |\n",
    "        | (IPC - Inter Process Communication)\n",
    "        v\n",
    "[JVM Process Memory]\n",
    "    - Spark Driver\n",
    "    - Java objects\n",
    "    - Execution plans\n",
    "    - Metadata\n",
    "```\n",
    "\n",
    "**Important:** Data exists in BOTH processes during certain operations!\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "| Component | Language | Purpose |\n",
    "|-----------|----------|---------|\n",
    "| PySpark Main | Python | User interface |\n",
    "| Py4J | Python/Java | Bridge |\n",
    "| Spark Driver | Java/Scala | Execution planning |\n",
    "| Spark Core | Scala | Core engine |\n",
    "| Executors | JVM | Task execution |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Diagram\n",
    "\n",
    "```text\n",
    "Full PySpark Execution Flow\n",
    "============================\n",
    "\n",
    "User Python Code\n",
    "    |\n",
    "    v\n",
    "PySpark API (Python)\n",
    "    |\n",
    "    v\n",
    "Py4J Serialization\n",
    "    |\n",
    "    v\n",
    "JVM Spark Driver (Scala/Java)\n",
    "    |\n",
    "    +-> DAG Creation\n",
    "    +-> Query Optimization (Catalyst)\n",
    "    +-> Job Creation\n",
    "    +-> Stage Division\n",
    "    +-> Task Generation\n",
    "    |\n",
    "    v\n",
    "Cluster Manager\n",
    "    |\n",
    "    v\n",
    "Executors (Distributed)\n",
    "    |\n",
    "    +-> Task Execution\n",
    "    +-> Data Processing\n",
    "    |\n",
    "    v\n",
    "Results\n",
    "    |\n",
    "    v\n",
    "Py4J Deserialization\n",
    "    |\n",
    "    v\n",
    "Python Results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **PySpark is a wrapper** around Spark's JVM engine\n",
    "2. **Py4J bridges** Python and JVM\n",
    "3. **Actual execution** happens in JVM (Scala/Java)\n",
    "4. **Performance loss** occurs at serialization boundaries\n",
    "5. **Avoid Python UDFs** when possible\n",
    "6. **Use built-in functions** for best performance\n",
    "7. **DataFrame API** is optimized regardless of language\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** PySpark gives you Python syntax with JVM performance, but minimize crossing the Python-JVM boundary for best results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d3bacb-08b2-4227-a1dd-5eafee779538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Lazy Evaluation & Actions in Apache Spark\n",
    "\n",
    "## What is Lazy Evaluation?\n",
    "\n",
    "**Lazy Evaluation** means:\n",
    "Spark does NOT execute transformations immediately.\n",
    "\n",
    "Spark only:\n",
    "- Records transformations\n",
    "- Builds a logical plan (DAG)\n",
    "- Waits until an ACTION is called\n",
    "\n",
    "**Execution starts ONLY when an ACTION is triggered.**\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Representation\n",
    "\n",
    "```text\n",
    "What You Think Happens:\n",
    "========================\n",
    "df.select() -> Execute -> Result\n",
    "df.filter() -> Execute -> Result\n",
    "df.groupBy() -> Execute -> Result\n",
    "\n",
    "What Actually Happens:\n",
    "========================\n",
    "df.select() -> Record in plan\n",
    "df.filter() -> Record in plan\n",
    "df.groupBy() -> Record in plan\n",
    "df.show() -> NOW EXECUTE ALL!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Transformations\n",
    "\n",
    "Transformations are operations that:\n",
    "- Create a new DataFrame / RDD\n",
    "- Do NOT trigger execution\n",
    "- Are lazily evaluated\n",
    "\n",
    "### Examples:\n",
    "```python\n",
    "# All these are transformations (NO execution)\n",
    "df.select(\"name\", \"age\")\n",
    "df.filter(df.age > 30)\n",
    "df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "df.groupBy(\"department\").count()\n",
    "df.join(df2, \"id\")\n",
    "```\n",
    "\n",
    "### Key Point:\n",
    "**None of these execute immediately!**\n",
    "\n",
    "---\n",
    "\n",
    "## Transformation Flow Example\n",
    "\n",
    "```python\n",
    "# User writes code\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "df2 = df1.select(\"name\", \"salary\")\n",
    "df3 = df2.filter(df2.salary > 50000)\n",
    "df4 = df3.groupBy(\"name\").count()\n",
    "```\n",
    "\n",
    "### What Spark is doing internally (NO execution yet):\n",
    "\n",
    "```text\n",
    "Step 1: df1 = spark.read.parquet(\"/data\")\n",
    "    Spark: \"Okay, I'll remember to read from /data\"\n",
    "    Status: NO execution\n",
    "\n",
    "Step 2: df2 = df1.select(\"name\", \"salary\")\n",
    "    Spark: \"Okay, I'll remember to select these columns\"\n",
    "    Status: NO execution\n",
    "\n",
    "Step 3: df3 = df2.filter(df2.salary > 50000)\n",
    "    Spark: \"Okay, I'll remember to filter\"\n",
    "    Status: NO execution\n",
    "\n",
    "Step 4: df4 = df3.groupBy(\"name\").count()\n",
    "    Spark: \"Okay, I'll remember to group and count\"\n",
    "    Status: STILL NO execution!\n",
    "```\n",
    "\n",
    "**Important:** Spark does NOT:\n",
    "- Read data\n",
    "- Compute results\n",
    "- Use any cluster resources\n",
    "- Spark only remembers the steps\n",
    "\n",
    "---\n",
    "\n",
    "## DAG (Directed Acyclic Graph)\n",
    "\n",
    "Spark builds a DAG internally to represent the execution plan.\n",
    "\n",
    "**DAG** = Directed Acyclic Graph = Flow of operations\n",
    "\n",
    "```text\n",
    "Logical Plan (DAG):\n",
    "===================\n",
    "\n",
    "Source Data (/data)\n",
    "     |\n",
    "     v\n",
    "select(name, salary)\n",
    "     |\n",
    "     v\n",
    "filter(salary > 50000)\n",
    "     |\n",
    "     v\n",
    "groupBy(name).count()\n",
    "     |\n",
    "     v\n",
    "[Waiting for ACTION...]\n",
    "```\n",
    "\n",
    "### What is DAG?\n",
    "- **Directed:** Operations flow in one direction\n",
    "- **Acyclic:** No loops, no circular dependencies\n",
    "- **Graph:** Series of connected operations\n",
    "\n",
    "---\n",
    "\n",
    "## Actions\n",
    "\n",
    "**Actions** are operations that:\n",
    "- Trigger execution\n",
    "- Return results to Driver\n",
    "- OR write data to storage\n",
    "\n",
    "### Common Actions:\n",
    "\n",
    "| Action | What it does |\n",
    "|--------|--------------|\n",
    "| show() | Display rows in console |\n",
    "| count() | Count number of rows |\n",
    "| collect() | Bring all data to driver |\n",
    "| take(n) | Bring first n rows to driver |\n",
    "| first() | Get first row |\n",
    "| write() | Write data to storage |\n",
    "| save() | Save to file system |\n",
    "| foreach() | Apply function to each row |\n",
    "\n",
    "---\n",
    "\n",
    "## Action Triggers Execution\n",
    "\n",
    "```python\n",
    "# All transformations (no execution)\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "df2 = df1.select(\"name\", \"salary\")\n",
    "df3 = df2.filter(df2.salary > 50000)\n",
    "df4 = df3.groupBy(\"name\").count()\n",
    "\n",
    "# ACTION - triggers execution\n",
    "df4.show()  # NOW everything executes!\n",
    "```\n",
    "\n",
    "### What happens when show() is called:\n",
    "\n",
    "```text\n",
    "1. Spark finalizes the DAG\n",
    "2. Catalyst optimizer optimizes the plan\n",
    "3. Creates physical execution plan\n",
    "4. Divides into Jobs, Stages, Tasks\n",
    "5. Executes on cluster\n",
    "6. Returns results to driver\n",
    "7. Displays in console\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Lazy Evaluation Flow\n",
    "\n",
    "```text\n",
    "User Code Flow:\n",
    "================\n",
    "\n",
    "Transformation 1 (select)\n",
    "      |\n",
    "      v\n",
    "Transformation 2 (filter)\n",
    "      |\n",
    "      v\n",
    "Transformation 3 (groupBy)\n",
    "      |\n",
    "      v\n",
    " (NO execution yet)\n",
    "      |\n",
    "      v\n",
    "   ACTION (show/count/write)\n",
    "      |\n",
    "      v\n",
    "Spark starts execution\n",
    "      |\n",
    "      v\n",
    "Optimizer runs\n",
    "      |\n",
    "      v\n",
    "Physical plan created\n",
    "      |\n",
    "      v\n",
    "Jobs, Stages, Tasks created\n",
    "      |\n",
    "      v\n",
    "Execute on cluster\n",
    "      |\n",
    "      v\n",
    "Results returned\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why Lazy Evaluation?\n",
    "\n",
    "### 1. Performance Optimization\n",
    "Spark can see the ENTIRE plan before execution and optimize it.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "df2 = df1.select(\"name\", \"age\", \"salary\", \"department\", \"city\")\n",
    "df3 = df2.filter(df2.age > 30)\n",
    "df4 = df3.select(\"name\", \"age\")  # Only need 2 columns!\n",
    "df4.show()\n",
    "```\n",
    "\n",
    "**Without lazy evaluation:**\n",
    "- Read all columns\n",
    "- Filter\n",
    "- Then select 2 columns\n",
    "\n",
    "**With lazy evaluation (Spark optimizes):**\n",
    "- Read ONLY \"name\" and \"age\" columns (column pruning)\n",
    "- Apply filter\n",
    "- Much faster!\n",
    "\n",
    "### 2. Avoids Unnecessary Computation\n",
    "```python\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "df2 = df1.filter(df1.status == \"active\")\n",
    "df3 = df1.filter(df1.status == \"inactive\")\n",
    "\n",
    "# If only df2 is used\n",
    "df2.count()  # Only df2 executes, df3 is never computed\n",
    "```\n",
    "\n",
    "### 3. Enables Query Optimization\n",
    "Spark's Catalyst optimizer can:\n",
    "- Predicate pushdown\n",
    "- Column pruning\n",
    "- Constant folding\n",
    "- Join reordering\n",
    "\n",
    "### 4. Reduces Disk and Memory Usage\n",
    "Only necessary data is read and processed.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Example with Optimization\n",
    "\n",
    "```python\n",
    "# Transformations (lazy)\n",
    "df = spark.read.parquet(\"/large_dataset\")  # 100 columns, 1TB data\n",
    "df = df.filter(col(\"year\") == 2024)\n",
    "df = df.filter(col(\"country\") == \"USA\")\n",
    "df = df.select(\"user_id\", \"sales\")\n",
    "df = df.groupBy(\"user_id\").agg(sum(\"sales\"))\n",
    "\n",
    "# Action (triggers execution)\n",
    "result = df.count()\n",
    "```\n",
    "\n",
    "### What Spark Optimizes:\n",
    "\n",
    "**Before Optimization (Naive approach):**\n",
    "```text\n",
    "1. Read all 100 columns, 1TB data\n",
    "2. Filter by year\n",
    "3. Filter by country\n",
    "4. Select 2 columns\n",
    "5. Group and aggregate\n",
    "```\n",
    "\n",
    "**After Optimization (Catalyst):**\n",
    "```text\n",
    "1. Read ONLY user_id, sales, year, country columns (column pruning)\n",
    "2. Push filters to storage layer (predicate pushdown)\n",
    "3. Read only 2024 USA data\n",
    "4. Group and aggregate\n",
    "Result: Read only 10GB instead of 1TB!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Transformations vs Actions\n",
    "\n",
    "### Transformations (Lazy):\n",
    "```python\n",
    "# None of these execute\n",
    "df.select(\"col1\")\n",
    "df.filter(df.col1 > 10)\n",
    "df.withColumn(\"new\", lit(1))\n",
    "df.groupBy(\"col1\").sum(\"col2\")\n",
    "df.join(df2, \"key\")\n",
    "df.distinct()\n",
    "df.orderBy(\"col1\")\n",
    "```\n",
    "\n",
    "### Actions (Eager):\n",
    "```python\n",
    "# All of these trigger execution\n",
    "df.show()\n",
    "df.count()\n",
    "df.collect()\n",
    "df.take(10)\n",
    "df.first()\n",
    "df.write.parquet(\"/output\")\n",
    "df.foreach(lambda x: print(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Production Examples\n",
    "\n",
    "### Example 1: Multiple Transformations, Single Action\n",
    "```python\n",
    "# Good: All transformations, single action\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.filter(col(\"active\") == True)\n",
    "df = df.select(\"user_id\", \"revenue\")\n",
    "df = df.groupBy(\"user_id\").agg(sum(\"revenue\"))\n",
    "df.write.parquet(\"/output\")  # Single action\n",
    "\n",
    "# Result: 1 job execution\n",
    "```\n",
    "\n",
    "### Example 2: Multiple Actions (Avoid)\n",
    "```python\n",
    "# Bad: Multiple actions\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.filter(col(\"active\") == True)\n",
    "\n",
    "count = df.count()  # Action 1 - Full execution\n",
    "df.show()           # Action 2 - Full execution again!\n",
    "df.write.parquet(\"/output\")  # Action 3 - Full execution again!\n",
    "\n",
    "# Result: 3 separate job executions!\n",
    "```\n",
    "\n",
    "### Example 3: Cache for Multiple Actions\n",
    "```python\n",
    "# Better: Cache when multiple actions needed\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.filter(col(\"active\") == True)\n",
    "df.cache()  # Cache the result\n",
    "\n",
    "count = df.count()  # Action 1 - Executes and caches\n",
    "df.show()           # Action 2 - Uses cache\n",
    "df.write.parquet(\"/output\")  # Action 3 - Uses cache\n",
    "\n",
    "# Result: 1 execution, 2 cache reads\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Check DAG\n",
    "\n",
    "### In Spark UI:\n",
    "1. Go to Spark UI (port 4040)\n",
    "2. Click on \"SQL\" or \"Jobs\" tab\n",
    "3. Click on a job\n",
    "4. See \"DAG Visualization\"\n",
    "\n",
    "### In Code:\n",
    "```python\n",
    "# See logical plan\n",
    "df.explain(True)\n",
    "\n",
    "# Output shows:\n",
    "# == Parsed Logical Plan ==\n",
    "# == Analyzed Logical Plan ==\n",
    "# == Optimized Logical Plan ==\n",
    "# == Physical Plan ==\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Optimization Examples\n",
    "\n",
    "### Example 1: Filter Pushdown\n",
    "```python\n",
    "# Code\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.select(\"name\", \"age\", \"city\")\n",
    "df = df.filter(df.age > 30)\n",
    "\n",
    "# Spark optimizes to:\n",
    "# Read parquet with filter age > 30 pushed down\n",
    "# Read only name, age, city columns\n",
    "```\n",
    "\n",
    "### Example 2: Column Pruning\n",
    "```python\n",
    "# Code\n",
    "df = spark.read.parquet(\"/data\")  # 50 columns\n",
    "df = df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Spark optimizes to:\n",
    "# Read ONLY col1 and col2 from parquet\n",
    "# (Doesn't read all 50 columns)\n",
    "```\n",
    "\n",
    "### Example 3: Predicate Pushdown\n",
    "```python\n",
    "# Code\n",
    "df = spark.read.jdbc(url, \"table\")\n",
    "df = df.filter(col(\"date\") == \"2024-01-01\")\n",
    "\n",
    "# Spark optimizes to:\n",
    "# SELECT * FROM table WHERE date = '2024-01-01'\n",
    "# (Filter happens in database, not in Spark)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Calling collect() on Large Data\n",
    "```python\n",
    "# BAD - Brings all data to driver\n",
    "data = df.collect()  # Can crash driver!\n",
    "\n",
    "# GOOD - Process in distributed manner\n",
    "df.write.parquet(\"/output\")\n",
    "```\n",
    "\n",
    "### Mistake 2: Multiple Actions Without Cache\n",
    "```python\n",
    "# BAD - Recomputes every time\n",
    "df.count()\n",
    "df.show()\n",
    "df.write.parquet(\"/output\")\n",
    "\n",
    "# GOOD - Cache first\n",
    "df.cache()\n",
    "df.count()\n",
    "df.show()\n",
    "df.write.parquet(\"/output\")\n",
    "```\n",
    "\n",
    "### Mistake 3: Unnecessary Intermediate Actions\n",
    "```python\n",
    "# BAD\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "print(f\"Count: {df1.count()}\")  # Action 1\n",
    "df2 = df1.filter(col(\"active\") == True)\n",
    "print(f\"Count: {df2.count()}\")  # Action 2\n",
    "result = df2.groupBy(\"user\").count()\n",
    "result.show()  # Action 3\n",
    "\n",
    "# GOOD\n",
    "df1 = spark.read.parquet(\"/data\")\n",
    "df2 = df1.filter(col(\"active\") == True)\n",
    "result = df2.groupBy(\"user\").count()\n",
    "result.show()  # Single action\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding with Timeline\n",
    "\n",
    "```text\n",
    "Time 0: Code starts\n",
    "==================\n",
    "df = spark.read.parquet(\"/data\")\n",
    "Status: No execution, just plan\n",
    "\n",
    "Time 1: More transformations\n",
    "=============================\n",
    "df = df.select(\"col1\", \"col2\")\n",
    "df = df.filter(df.col1 > 10)\n",
    "Status: Still no execution, plan grows\n",
    "\n",
    "Time 2: Even more transformations\n",
    "==================================\n",
    "df = df.groupBy(\"col1\").count()\n",
    "Status: Still no execution, plan continues to grow\n",
    "\n",
    "Time 3: ACTION called\n",
    "=====================\n",
    "df.show()\n",
    "Status: NOW execution starts!\n",
    "- Optimizer runs\n",
    "- Physical plan created\n",
    "- Jobs launched\n",
    "- Cluster resources used\n",
    "- Results returned\n",
    "\n",
    "Time 4: Complete\n",
    "================\n",
    "Results displayed\n",
    "Execution finished\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "| Concept | Meaning |\n",
    "|---------|---------|\n",
    "| Lazy Evaluation | Delay execution until action |\n",
    "| Transformation | Create new DF, no execution |\n",
    "| Action | Trigger execution |\n",
    "| DAG | Logical plan of operations |\n",
    "| Catalyst | Spark's optimizer |\n",
    "| Physical Plan | Actual execution strategy |\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits Recap\n",
    "\n",
    "1. **Optimization:** Spark sees entire plan, optimizes globally\n",
    "2. **Efficiency:** Only necessary work is done\n",
    "3. **Resource Savings:** Read only needed data\n",
    "4. **Performance:** Push filters/projections to source\n",
    "5. **Smart Execution:** Combines operations when possible\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Decision Tree\n",
    "\n",
    "```text\n",
    "Should I use .cache() or .persist()?\n",
    "=====================================\n",
    "\n",
    "Will you use this DataFrame multiple times?\n",
    "    |\n",
    "    +-- NO -> Don't cache (waste of memory)\n",
    "    |\n",
    "    +-- YES -> How many times?\n",
    "              |\n",
    "              +-- 2 times -> Maybe cache\n",
    "              |\n",
    "              +-- 3+ times -> Definitely cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Final Example: Complete Flow\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Start: All lazy transformations\n",
    "df = spark.read.parquet(\"/sales_data\")  # Not executed\n",
    "df = df.filter(col(\"year\") == 2024)     # Not executed\n",
    "df = df.filter(col(\"region\") == \"USA\")  # Not executed\n",
    "df = df.select(\"product\", \"revenue\")    # Not executed\n",
    "df = df.groupBy(\"product\").agg(sum(\"revenue\"))  # Not executed\n",
    "\n",
    "# At this point: ZERO execution, ZERO data read\n",
    "# Spark only has a plan (DAG)\n",
    "\n",
    "# Now action\n",
    "df.write.parquet(\"/output\")  # EXECUTES EVERYTHING\n",
    "\n",
    "# What Spark does:\n",
    "# 1. Optimizes: Read only year, region, product, revenue\n",
    "# 2. Pushes filters: year=2024 AND region=USA to storage\n",
    "# 3. Reads only filtered data\n",
    "# 4. Groups and aggregates\n",
    "# 5. Writes output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Remember\n",
    "\n",
    "**Transformations are lazy, Actions are eager!**\n",
    "\n",
    "Every transformation just adds to the plan.\n",
    "Every action executes the entire plan.\n",
    "\n",
    "**Optimize by:**\n",
    "- Minimizing actions\n",
    "- Using cache when needed\n",
    "- Letting Spark optimize the full plan\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:** Lazy evaluation allows Spark to be smart about execution. Trust it, use it, benefit from it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e17b801-9eb3-4cf6-bc69-8df92caef77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afc25ec-41e4-464c-8eb2-d3bcde1e8c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to make Spark Session (Databricks)\n",
    "\n",
    "```python\n",
    "# SparkSession is already available as `spark` in Databricks\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5f7d97-38f7-4212-944c-64fc2d785a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Partitions & RDD in Apache Spark\n",
    "\n",
    "## What is a Partition?\n",
    "\n",
    "- A partition is a small chunk of data\n",
    "- Spark divides large data into multiple partitions\n",
    "- Each partition is processed independently\n",
    "\n",
    "### Key Points:\n",
    "- One partition = one task\n",
    "- One task runs on one executor core\n",
    "- More partitions = more parallelism\n",
    "\n",
    "### Visual Representation:\n",
    "```text\n",
    "Large Dataset (1 TB)\n",
    "        |\n",
    "        v\n",
    "Divided into partitions\n",
    "        |\n",
    "        v\n",
    "[P1] [P2] [P3] [P4] [P5] [P6] [P7] [P8]\n",
    " |    |    |    |    |    |    |    |\n",
    " v    v    v    v    v    v    v    v\n",
    "Task Task Task Task Task Task Task Task\n",
    " |    |    |    |    |    |    |    |\n",
    " v    v    v    v    v    v    v    v\n",
    "Core Core Core Core Core Core Core Core\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why Partitions are Important?\n",
    "\n",
    "Partitions decide:\n",
    "- Parallel execution\n",
    "- Job performance\n",
    "- Resource utilization\n",
    "- Execution speed\n",
    "\n",
    "**Bad partitioning = slow job**  \n",
    "**Good partitioning = fast job**\n",
    "\n",
    "### Example Impact:\n",
    "```text\n",
    "Scenario 1: Too Few Partitions (2 partitions, 8 cores)\n",
    "[P1======] [P2======]\n",
    "Core1      Core2      Core3 Core4 Core5 Core6 Core7 Core8\n",
    "(busy)     (busy)     (idle)(idle)(idle)(idle)(idle)(idle)\n",
    "Result: 6 cores wasted! Poor performance.\n",
    "\n",
    "Scenario 2: Good Partitions (8 partitions, 8 cores)\n",
    "[P1] [P2] [P3] [P4] [P5] [P6] [P7] [P8]\n",
    "Core1 Core2 Core3 Core4 Core5 Core6 Core7 Core8\n",
    "Result: All cores utilized! Good performance.\n",
    "\n",
    "Scenario 3: Too Many Partitions (100 partitions, 8 cores)\n",
    "[P1][P2][P3]...[P100]\n",
    "Result: Too much scheduling overhead! Slow performance.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What is RDD?\n",
    "\n",
    "**RDD** stands for **Resilient Distributed Dataset**.\n",
    "\n",
    "### Meaning:\n",
    "- **Resilient** = fault tolerant (can recover from failures)\n",
    "- **Distributed** = spread across multiple machines\n",
    "- **Dataset** = collection of data\n",
    "\n",
    "RDD is the fundamental data structure of Spark.\n",
    "\n",
    "### RDD Characteristics:\n",
    "- Immutable (cannot be changed once created)\n",
    "- Lazily evaluated (computed only when action is called)\n",
    "- Partitioned across cluster\n",
    "- Fault-tolerant through lineage\n",
    "\n",
    "---\n",
    "\n",
    "## Relationship Between RDD and Partition\n",
    "\n",
    "- An RDD is made up of multiple partitions\n",
    "- Data inside an RDD is always partitioned\n",
    "- Spark processes RDD partition by partition\n",
    "\n",
    "### Visual Diagram:\n",
    "```text\n",
    "         RDD (Logical View)\n",
    "              |\n",
    "              v\n",
    "     [Complete Dataset]\n",
    "              |\n",
    "              v\n",
    "    Physical Distribution\n",
    "              |\n",
    "    +---------+---------+\n",
    "    |         |         |\n",
    "    v         v         v\n",
    "Partition 1  Partition 2  Partition 3\n",
    "(on Node A)  (on Node B)  (on Node C)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How Spark Executes Using Partitions\n",
    "\n",
    "```text\n",
    "RDD with 4 Partitions\n",
    "        |\n",
    "        +---> Partition 1 -> Task 1 -> Executor Core 1\n",
    "        |\n",
    "        +---> Partition 2 -> Task 2 -> Executor Core 2\n",
    "        |\n",
    "        +---> Partition 3 -> Task 3 -> Executor Core 3\n",
    "        |\n",
    "        +---> Partition 4 -> Task 4 -> Executor Core 4\n",
    "\n",
    "Each partition is processed in parallel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important Execution Rules\n",
    "\n",
    "1. Spark never processes full data at once\n",
    "2. Spark always processes data partition-wise\n",
    "3. Number of partitions = number of tasks\n",
    "4. Tasks run in parallel on available cores\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "# Check number of partitions\n",
    "df = spark.read.parquet(\"/data\")\n",
    "print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# This creates 8 tasks (one per partition)\n",
    "df.rdd.getNumPartitions()  # Output: 8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## RDD vs DataFrame (Basic Understanding)\n",
    "\n",
    "| Aspect | RDD | DataFrame |\n",
    "|--------|-----|-----------|\n",
    "| Level | Low-level | High-level |\n",
    "| Optimization | Manual | Automatic (Catalyst) |\n",
    "| Type Safety | Compile-time | Runtime |\n",
    "| Ease of Use | Complex | Easy |\n",
    "| Performance | Good | Better (optimized) |\n",
    "| API | Functional | SQL + Functional |\n",
    "\n",
    "### Relationship:\n",
    "```text\n",
    "DataFrame (High-level API)\n",
    "        |\n",
    "        v\n",
    "   Uses internally\n",
    "        |\n",
    "        v\n",
    "RDD (Low-level execution engine)\n",
    "        |\n",
    "        v\n",
    "Partitions (Physical data chunks)\n",
    "```\n",
    "\n",
    "**Note:** DataFrame internally uses RDD but adds optimization layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "### 1. Optimal Partition Size\n",
    "**Rule of Thumb:** Each partition should be 100-200 MB\n",
    "\n",
    "```python\n",
    "# Calculate ideal partitions\n",
    "data_size_gb = 100  # GB\n",
    "partition_size_mb = 128  # MB\n",
    "ideal_partitions = (data_size_gb * 1024) / partition_size_mb\n",
    "print(f\"Ideal partitions: {ideal_partitions}\")  # ~800 partitions\n",
    "```\n",
    "\n",
    "### 2. Partition Count\n",
    "**Rule:** Number of partitions = 2-3x number of cores in cluster\n",
    "\n",
    "```python\n",
    "# If you have 100 cores\n",
    "ideal_partitions = 100 * 3  # 300 partitions\n",
    "df = df.repartition(300)\n",
    "```\n",
    "\n",
    "### 3. Check Partition Sizes\n",
    "```python\n",
    "# Check partition distribution\n",
    "partition_sizes = df.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")\n",
    "\n",
    "# Check for data skew\n",
    "max_size = max(partition_sizes)\n",
    "min_size = min(partition_sizes)\n",
    "if max_size > min_size * 2:\n",
    "    print(\"Warning: Data skew detected!\")\n",
    "```\n",
    "\n",
    "### 4. When to Repartition\n",
    "```python\n",
    "# BEFORE wide transformations\n",
    "df = spark.read.parquet(\"/large_data\")\n",
    "df = df.repartition(200, \"user_id\")  # Partition by key\n",
    "result = df.groupBy(\"user_id\").agg(...)  # Now faster\n",
    "```\n",
    "\n",
    "### 5. When to Coalesce\n",
    "```python\n",
    "# AFTER filtering\n",
    "df_large = spark.read.parquet(\"/data\")  # 1000 partitions\n",
    "df_filtered = df_large.filter(col(\"active\") == True)  # 90% data removed\n",
    "df_optimized = df_filtered.coalesce(100)  # Reduce partitions\n",
    "df_optimized.write.parquet(\"/output\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Production Problems\n",
    "\n",
    "### Problem 1: Too Few Partitions\n",
    "**Symptom:** Some cores idle, job takes long time\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Increase partitions\n",
    "df = df.repartition(num_cores * 3)\n",
    "```\n",
    "\n",
    "### Problem 2: Too Many Partitions\n",
    "**Symptom:** High scheduling overhead, many small tasks\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Reduce partitions\n",
    "df = df.coalesce(optimal_count)\n",
    "```\n",
    "\n",
    "### Problem 3: Data Skew\n",
    "**Symptom:** Most tasks finish fast, few tasks take forever\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Add salt to skewed key\n",
    "from pyspark.sql.functions import rand, concat, lit\n",
    "\n",
    "df = df.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "df = df.withColumn(\"salted_key\", concat(col(\"user_id\"), lit(\"_\"), col(\"salt\")))\n",
    "df = df.repartition(\"salted_key\")\n",
    "```\n",
    "\n",
    "### Problem 4: Small Files Problem\n",
    "**Symptom:** Thousands of tiny output files\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Coalesce before writing\n",
    "df.coalesce(10).write.parquet(\"/output\")  # Creates 10 files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced: Partition Control\n",
    "\n",
    "### 1. Default Partitioning\n",
    "```python\n",
    "# Spark decides based on file size\n",
    "df = spark.read.parquet(\"/data\")\n",
    "# Default: ~128 MB per partition\n",
    "```\n",
    "\n",
    "### 2. Custom Partitioning\n",
    "```python\n",
    "# Explicit partition count\n",
    "df = spark.read.option(\"spark.sql.files.maxPartitionBytes\", \"256MB\") \\\n",
    "          .parquet(\"/data\")\n",
    "```\n",
    "\n",
    "### 3. Range Partitioning\n",
    "```python\n",
    "# Partition by value ranges\n",
    "df = df.repartitionByRange(10, \"user_id\")\n",
    "# Good for sorted data\n",
    "```\n",
    "\n",
    "### 4. Hash Partitioning\n",
    "```python\n",
    "# Partition by hash of column\n",
    "df = df.repartition(20, \"user_id\")\n",
    "# Ensures same user_id in same partition\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring Partitions\n",
    "\n",
    "### In Spark UI:\n",
    "1. Go to Spark UI (usually port 4040)\n",
    "2. Check \"Stages\" tab\n",
    "3. Look at \"Number of Tasks\"\n",
    "4. Each task = one partition\n",
    "\n",
    "### In Code:\n",
    "```python\n",
    "# Get partition count\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# See data distribution\n",
    "df.rdd.glom().map(len).collect()\n",
    "\n",
    "# Check partition details\n",
    "df.explain()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Example: Optimizing Partitions\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Step 1: Read data (default partitioning)\n",
    "df = spark.read.parquet(\"/large_dataset\")\n",
    "print(f\"Initial partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Step 2: Filter reduces data significantly\n",
    "df_filtered = df.filter(col(\"year\") == 2024)\n",
    "\n",
    "# Step 3: Coalesce after filtering\n",
    "df_filtered = df_filtered.coalesce(50)\n",
    "\n",
    "# Step 4: Repartition by key before groupBy\n",
    "df_prepared = df_filtered.repartition(100, \"department\")\n",
    "\n",
    "# Step 5: Perform aggregation (benefits from partitioning)\n",
    "result = df_prepared.groupBy(\"department\").agg(\n",
    "    sum(\"sales\").alias(\"total_sales\")\n",
    ")\n",
    "\n",
    "# Step 6: Coalesce before writing\n",
    "result.coalesce(5).write.mode(\"overwrite\").parquet(\"/output\")\n",
    "\n",
    "print(\"Optimization complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Formulas to Remember\n",
    "\n",
    "```text\n",
    "Ideal Partitions = Data Size (MB) / Target Partition Size (128 MB)\n",
    "\n",
    "Or\n",
    "\n",
    "Ideal Partitions = Number of Cores Ã— 2 to 3\n",
    "\n",
    "Partition Size Check = Total Data Size / Number of Partitions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## One-Line Summary\n",
    "\n",
    "RDD is a distributed dataset divided into partitions, and Spark processes each partition in parallel using one task per partition.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference Card\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|-----------|\n",
    "| Partition | Small chunk of data |\n",
    "| RDD | Distributed collection divided into partitions |\n",
    "| Task | Processing unit for one partition |\n",
    "| Narrow Transform | No shuffle, same partition count |\n",
    "| Wide Transform | Shuffle occurs, new partition count |\n",
    "| Repartition | Increase or decrease with shuffle |\n",
    "| Coalesce | Decrease only, no shuffle |\n",
    "| Optimal Size | 100-200 MB per partition |\n",
    "| Optimal Count | 2-3x number of cores |\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Good partitioning = Good performance. Always monitor and optimize partition count and size in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbfee8b1-9051-405c-a148-dcf5fc436140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Transformations in Apache Spark\n",
    "\n",
    "## What is a Transformation?\n",
    "\n",
    "- A transformation is an operation that creates a new RDD or DataFrame\n",
    "- Transformations are lazily evaluated\n",
    "- Execution does NOT happen until an action is called\n",
    "\n",
    "Examples:\n",
    "- select()\n",
    "- filter()\n",
    "- map()\n",
    "- groupBy()\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Transformations\n",
    "\n",
    "Spark transformations are classified into two types:\n",
    "1. Narrow Transformations\n",
    "2. Wide Transformations\n",
    "\n",
    "---\n",
    "\n",
    "## Narrow Transformation\n",
    "\n",
    "### Definition\n",
    "A narrow transformation is one where:\n",
    "- Each output partition depends on only ONE input partition\n",
    "- No data movement across partitions\n",
    "- No shuffle occurs\n",
    "\n",
    "### Partition Flow\n",
    "```text\n",
    "Partition 1 -> Transformation -> Partition 1\n",
    "Partition 2 -> Transformation -> Partition 2\n",
    "Partition 3 -> Transformation -> Partition 3\n",
    "```\n",
    "\n",
    "### Characteristics\n",
    "- Fast execution\n",
    "- One-to-one partition dependency\n",
    "- Same stage execution\n",
    "- No network overhead\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "# Narrow transformations\n",
    "df.select(\"name\", \"age\")\n",
    "df.filter(df.age > 30)\n",
    "df.withColumn(\"new_col\", col(\"salary\") * 2)\n",
    "df.drop(\"unwanted_column\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Wide Transformation\n",
    "\n",
    "### Definition\n",
    "A wide transformation is one where:\n",
    "- Output partitions depend on MULTIPLE input partitions\n",
    "- Data moves across partitions\n",
    "- Shuffle occurs\n",
    "\n",
    "### Partition Flow\n",
    "```text\n",
    "Partition 1 \\\n",
    "Partition 2  \\\n",
    "              -> Shuffle -> New Partition A\n",
    "Partition 3  /\n",
    "Partition 4 /\n",
    "\n",
    "Data is redistributed across the cluster\n",
    "```\n",
    "\n",
    "### Characteristics\n",
    "- Slower execution\n",
    "- Many-to-one partition dependency\n",
    "- Creates new stage\n",
    "- Network overhead (shuffle)\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "# Wide transformations\n",
    "df.groupBy(\"department\").count()\n",
    "df.orderBy(\"salary\")\n",
    "df1.join(df2, \"user_id\")\n",
    "df.distinct()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Transformations: Narrow vs Wide\n",
    "\n",
    "### Narrow Transformations (No Shuffle)\n",
    "\n",
    "These transformations operate on data within the same partition.\n",
    "\n",
    "**Functions:**\n",
    "- select()\n",
    "- filter()\n",
    "- map()\n",
    "- withColumn()\n",
    "- drop()\n",
    "- limit()\n",
    "- sample()\n",
    "- union()\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# All these are narrow transformations\n",
    "df.select(\"name\", \"department\") \\\n",
    "  .filter(df.salary > 50000) \\\n",
    "  .withColumn(\"bonus\", col(\"salary\") * 0.1) \\\n",
    "  .drop(\"temp_column\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Wide Transformations (Shuffle Happens)\n",
    "\n",
    "These transformations require data movement across partitions.\n",
    "\n",
    "**Functions:**\n",
    "- groupBy()\n",
    "- reduceByKey()\n",
    "- aggregateByKey()\n",
    "- join()\n",
    "- distinct()\n",
    "- orderBy()\n",
    "- sort()\n",
    "- repartition()\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# All these are wide transformations\n",
    "df.groupBy(\"department\").agg(sum(\"salary\")) \\\n",
    "  .orderBy(\"department\") \\\n",
    "  .distinct()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Aspect | Narrow Transformation | Wide Transformation |\n",
    "|--------|----------------------|---------------------|\n",
    "| Shuffle | No shuffle | Shuffle happens |\n",
    "| Speed | Faster | Slower |\n",
    "| Dependency | One-to-one | Many-to-one |\n",
    "| Stage | Same stage | Creates new stage |\n",
    "| Network | No network transfer | Network transfer required |\n",
    "| Examples | select, filter, map | groupBy, join, orderBy |\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Impact\n",
    "\n",
    "### Narrow Transformations\n",
    "- Minimal overhead\n",
    "- Can be pipelined together\n",
    "- Execute in memory\n",
    "- No disk I/O for shuffle\n",
    "\n",
    "### Wide Transformations\n",
    "- High overhead\n",
    "- Requires disk write/read\n",
    "- Network data transfer\n",
    "- Creates shuffle files\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "### DO:\n",
    "- Chain multiple narrow transformations together\n",
    "- Use filter() early to reduce data size before wide transformations\n",
    "- Minimize the number of wide transformations\n",
    "- Use broadcast joins for small tables\n",
    "\n",
    "### AVOID:\n",
    "- Unnecessary groupBy() or distinct()\n",
    "- Multiple joins in sequence without optimization\n",
    "- orderBy() on large datasets unless required\n",
    "\n",
    "---\n",
    "\n",
    "## Full Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(\"/data\")\n",
    "\n",
    "# Narrow transformations (fast, no shuffle)\n",
    "df_filtered = df.select(\"user_id\", \"department\", \"salary\") \\\n",
    "                .filter(col(\"salary\") > 50000) \\\n",
    "                .withColumn(\"tax\", col(\"salary\") * 0.3)\n",
    "\n",
    "# Wide transformation (shuffle happens, creates new stage)\n",
    "result = df_filtered.groupBy(\"department\") \\\n",
    "                    .agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "\n",
    "# Another wide transformation\n",
    "final = result.orderBy(\"total_salary\", ascending=False)\n",
    "\n",
    "# Action triggers execution\n",
    "final.show()\n",
    "```\n",
    "\n",
    "### Execution breakdown:\n",
    "1. select, filter, withColumn = Narrow (Stage 1)\n",
    "2. groupBy = Wide (creates Stage 2, shuffle occurs)\n",
    "3. orderBy = Wide (creates Stage 3, shuffle occurs)\n",
    "4. show() = Action (triggers all stages)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Narrow transformations are fast and efficient\n",
    "- Wide transformations cause shuffle and are expensive\n",
    "- Minimize wide transformations for better performance\n",
    "- Use filters early to reduce data before shuffles\n",
    "- Every wide transformation creates a new stage\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Narrow = No Shuffle = Fast | Wide = Shuffle = Slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1e9f97-1bd4-4fa3-bb97-ae85a1d08439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# repartition() vs coalesce() in Apache Spark\n",
    "\n",
    "## Why do we need them?\n",
    "- Spark divides data into partitions\n",
    "- Each partition is processed by one executor core\n",
    "- Too few partitions = resources are wasted\n",
    "- Too many partitions = overhead and slow performance\n",
    "\n",
    "So we adjust partitions using `repartition()` and `coalesce()`.\n",
    "\n",
    "---\n",
    "\n",
    "## repartition()\n",
    "\n",
    "**What it does:**  \n",
    "Reshuffles data to change the number of partitions.\n",
    "\n",
    "### Key Points\n",
    "- Causes shuffle (expensive)\n",
    "- Can INCREASE or DECREASE partitions\n",
    "- Creates balanced partitions\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "# Original DataFrame\n",
    "df = spark.range(1000)\n",
    "\n",
    "# Increase partitions for better parallelism\n",
    "df_new = df.repartition(8)\n",
    "\n",
    "# Repartition by column (useful before join/groupBy)\n",
    "df_by_dept = df.repartition(4, \"department\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## coalesce()\n",
    "\n",
    "**What it does:**  \n",
    "Merges existing partitions to reduce their count without reshuffling data.\n",
    "\n",
    "### Key Points\n",
    "- No shuffle (fast)\n",
    "- Can ONLY decrease partitions\n",
    "- May create uneven partitions\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "# Start with more partitions\n",
    "df = spark.range(1000).repartition(8)\n",
    "\n",
    "# Reduce partitions quickly\n",
    "df_small = df.coalesce(2)\n",
    "\n",
    "# Before writing output files\n",
    "df.coalesce(1).write.parquet(\"/path\")  # Creates a single file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When to use repartition()\n",
    "\n",
    "- When you need MORE partitions for parallel processing\n",
    "- Before wide operations like join or groupBy\n",
    "- When even data distribution is required\n",
    "\n",
    "---\n",
    "\n",
    "## When to use coalesce()\n",
    "\n",
    "- After filtering when data size is reduced\n",
    "- Before writing data to disk (to reduce number of files)\n",
    "- When you want to reduce partitions quickly with low cost\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Rules to Remember\n",
    "\n",
    "- Need more partitions? Use repartition()\n",
    "- Need fewer partitions? Use coalesce()\n",
    "- Before join or groupBy? Use repartition(column)\n",
    "- Before writing files? Use coalesce()\n",
    "\n",
    "---\n",
    "\n",
    "## Full Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Load data\n",
    "df = spark.read.parquet(\"/data\")\n",
    "\n",
    "# 2. Repartition by key before aggregation\n",
    "df = df.repartition(10, \"user_id\")\n",
    "\n",
    "# 3. Perform wide transformation\n",
    "result = df.groupBy(\"user_id\").count()\n",
    "\n",
    "# 4. Filter reduces data size\n",
    "result = result.filter(col(\"count\") > 100)\n",
    "\n",
    "# 5. Reduce partitions before writing\n",
    "result.coalesce(1).write.parquet(\"/output\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Comparison\n",
    "\n",
    "| Feature | repartition() | coalesce() |\n",
    "|---------|---------------|------------|\n",
    "| Shuffle? | Yes | No |\n",
    "| Speed | Slow | Fast |\n",
    "| Direction | Up or Down | Only Down |\n",
    "| Distribution | Even | May be uneven |\n",
    "| Use case | Before joins/groupBy | Before writing files |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:** Use repartition() when you need balanced partitions or more parallelism. Use coalesce() when you just want to reduce partitions quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "850f53ed-9c24-4bdf-9dde-1c55fa51e28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import *\n",
    "## API for Read the file.  and that option is fdor extra config that we want to pass , after that the  thigns is used to define a schema without defining a schema  it means it will predict the best schema for your data frame\n",
    "\n",
    "# 1 job = 1 stage + 1 task( Always) atleast\n",
    "df = spark.read.format(\"CSV\")\\  \n",
    "    .option(\"header\" , True)\\\n",
    "        .option(\"inferSchema\" , True)\\\n",
    "            .load(\"/FileStore/tables/StudentDataExample.csv\")\n",
    "\n",
    "\n",
    "# Transformations \n",
    "\n",
    "df = df.filter(col('prodcut_name') == 'Sneakers')\n",
    "\n",
    "# only select order id and prouct name \n",
    "df = df.select('order_id', 'prodcut_name')\n",
    "\n",
    "\n",
    "#group by ,  performing wide transformation \n",
    " df = df.groupBy('product_name').agg(count(col('order_id') == ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ed61b8-e9be-433c-8d81-2144917a7029",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Jobs, Stages, and Tasks in Apache Spark"
    }
   },
   "source": [
    "# Jobs, Stages, and Tasks in Apache Spark\n",
    "\n",
    "This section explains how Spark actually executes code internally.\n",
    "Understanding this is critical for performance tuning and debugging in production.\n",
    "\n",
    "---\n",
    "\n",
    "## Big Picture\n",
    "\n",
    "Spark executes work in the following hierarchy:\n",
    "\n",
    "**Job -> Stage -> Task**\n",
    "\n",
    "- **Job** answers: WHY Spark runs\n",
    "- **Stage** answers: HOW work is divided (shuffle or no shuffle)\n",
    "- **Task** answers: WHO does the actual work\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Job?\n",
    "\n",
    "### Definition\n",
    "A **Job** is created whenever an **ACTION** is called in Spark.\n",
    "\n",
    "Actions include:\n",
    "- show()\n",
    "- count()\n",
    "- collect()\n",
    "- write()\n",
    "\n",
    "### Example\n",
    "```python\n",
    "df.filter(df.age > 30).count()\n",
    "# This creates ONE Job\n",
    "```\n",
    "\n",
    "### Key Points\n",
    "- No action = no job\n",
    "- One action = one job\n",
    "- Multiple actions = multiple jobs\n",
    "\n",
    "### Production Note\n",
    "Too many actions in code = multiple jobs = slower pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Stage?\n",
    "\n",
    "A **Stage** is a set of transformations that can be executed without shuffle.\n",
    "\n",
    "- **Narrow transformations** stay in the same stage\n",
    "- **Wide transformations** (shuffle) create a new stage\n",
    "\n",
    "### Example\n",
    "```python\n",
    "df.select(\"name\") \\\n",
    "  .filter(df.salary > 50000) \\\n",
    "  .groupBy(\"department\") \\\n",
    "  .count()\n",
    "```\n",
    "\n",
    "### Spark creates:\n",
    "- **Stage 1** - select + filter (no shuffle)\n",
    "- **Stage 2** - groupBy + count (shuffle)\n",
    "\n",
    "### Key Points\n",
    "- Stages are divided by shuffle boundaries\n",
    "- More shuffles = more stages = slower jobs\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Task?\n",
    "\n",
    "A **Task** is the smallest unit of execution in Spark.\n",
    "\n",
    "### Important rule:\n",
    "**One partition = one task**\n",
    "\n",
    "### Example\n",
    "If a stage has 8 partitions, Spark creates 8 tasks.\n",
    "\n",
    "These tasks run in parallel on executor cores.\n",
    "\n",
    "### Key Points\n",
    "- Tasks run on executors\n",
    "- Tasks do the actual data processing\n",
    "- Number of tasks depends on number of partitions\n",
    "\n",
    "---\n",
    "\n",
    "## Full Execution Flow Example\n",
    "\n",
    "```python\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df2 = df.filter(df.age > 30)          # Narrow transformation\n",
    "df3 = df2.groupBy(\"dept\").count()     # Wide transformation (shuffle)\n",
    "df3.write.parquet(\"/output\")          # Action\n",
    "```\n",
    "\n",
    "### Spark execution:\n",
    "1. write() triggers ONE Job\n",
    "2. filter() runs in Stage 1\n",
    "3. groupBy() causes shuffle - creates Stage 2\n",
    "4. Each stage is executed using tasks (one per partition)\n",
    "\n",
    "---\n",
    "\n",
    "## Relationship Summary\n",
    "\n",
    "| Concept | Creates |\n",
    "|---------|---------|\n",
    "| Action | Job |\n",
    "| Shuffle | New Stage |\n",
    "| Partition | Task |\n",
    "\n",
    "---\n",
    "\n",
    "## Common Production Mistakes\n",
    "\n",
    "### 1. Too many actions\n",
    "```python\n",
    "df.count()\n",
    "df.show()\n",
    "df.write.parquet(\"/out\")\n",
    "# Creates 3 jobs (avoid if possible)\n",
    "```\n",
    "\n",
    "### 2. Too many shuffles\n",
    "- Unnecessary groupBy\n",
    "- Bad joins\n",
    "- Wrong repartition usage\n",
    "- Results in slow performance\n",
    "\n",
    "### 3. Bad partitioning\n",
    "- Too few partitions = CPU underutilized\n",
    "- Too many partitions = scheduling overhead\n",
    "\n",
    "---\n",
    "\n",
    "## What to Remember (Very Important)\n",
    "\n",
    "- Every action creates a job\n",
    "- Every shuffle creates a new stage\n",
    "- Every partition creates a task\n",
    "\n",
    "### Performance depends on:\n",
    "- Number of shuffles\n",
    "- Number of partitions\n",
    "- Number of actions\n",
    "\n",
    "---\n",
    "\n",
    "**Optimize by reducing shuffles and actions, and tuning partition count!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "879562d3-7bc9-4fc2-96ae-6bad0f7b71a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b28b64-3528-42ea-9c66-b52c61aac34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Shuffle Join vs Broadcast Join in Apache Spark\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Joins are one of the most expensive operations in Spark. Understanding when to use Shuffle Join vs Broadcast Join can save you **thousands of dollars** in cloud costs and **hours** in execution time.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Join?\n",
    "\n",
    "A join combines two tables based on a common key.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Customers table\n",
    "customers: customer_id, name, country\n",
    "\n",
    "# Orders table  \n",
    "orders: order_id, customer_id, amount\n",
    "\n",
    "# Join to get customer name with each order\n",
    "result = orders.join(customers, \"customer_id\")\n",
    "```\n",
    "\n",
    "**The Problem:**\n",
    "- Tables are distributed across multiple partitions\n",
    "- Matching rows might be on different machines\n",
    "- How do we bring them together?\n",
    "\n",
    "**Two Solutions:**\n",
    "1. Shuffle Join (move both tables)\n",
    "2. Broadcast Join (copy small table to everyone)\n",
    "\n",
    "---\n",
    "\n",
    "## Shuffle Join (Sort-Merge Join)\n",
    "\n",
    "### What is Shuffle Join?\n",
    "\n",
    "Shuffle Join redistributes data across the network so that rows with the same join key end up on the same partition.\n",
    "\n",
    "### Visual Explanation:\n",
    "\n",
    "```text\n",
    "BEFORE SHUFFLE:\n",
    "===============\n",
    "\n",
    "Table A (Customers)          Table B (Orders)\n",
    "Partition 1:                 Partition 1:\n",
    "  customer_id=1                order: customer_id=5\n",
    "  customer_id=2                order: customer_id=6\n",
    "\n",
    "Partition 2:                 Partition 2:\n",
    "  customer_id=3                order: customer_id=1\n",
    "  customer_id=4                order: customer_id=2\n",
    "\n",
    "Problem: customer_id=1 is in different partitions!\n",
    "\n",
    "\n",
    "AFTER SHUFFLE (Data Movement):\n",
    "===============================\n",
    "\n",
    "Table A (Customers)          Table B (Orders)\n",
    "Partition 1:                 Partition 1:\n",
    "  customer_id=1                order: customer_id=1  <- Now together!\n",
    "  customer_id=3                order: customer_id=3\n",
    "\n",
    "Partition 2:                 Partition 2:\n",
    "  customer_id=2                order: customer_id=2  <- Now together!\n",
    "  customer_id=4                order: customer_id=4\n",
    "\n",
    "Now: Join happens locally in each partition\n",
    "```\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "```text\n",
    "Step 1: Hash the join key\n",
    "    Spark calculates hash(customer_id) for both tables\n",
    "\n",
    "Step 2: Shuffle data\n",
    "    Rows with same hash go to same partition\n",
    "    Data moves across network (expensive!)\n",
    "\n",
    "Step 3: Sort data in each partition\n",
    "    Sort by join key\n",
    "\n",
    "Step 4: Merge join\n",
    "    Walk through sorted data and match rows\n",
    "\n",
    "Step 5: Return results\n",
    "```\n",
    "\n",
    "### Code Example:\n",
    "\n",
    "```python\n",
    "# Both tables are large\n",
    "customers = spark.read.parquet(\"/customers\")  # 1M rows, 2 GB\n",
    "orders = spark.read.parquet(\"/orders\")        # 100M rows, 50 GB\n",
    "\n",
    "# Shuffle Join happens (default for large tables)\n",
    "result = customers.join(orders, \"customer_id\")\n",
    "\n",
    "# What Spark does internally:\n",
    "# 1. Shuffle 2 GB of customers data\n",
    "# 2. Shuffle 50 GB of orders data\n",
    "# 3. Total shuffle: 52 GB across network!\n",
    "# 4. Creates 52 GB of shuffle files on disk\n",
    "```\n",
    "\n",
    "### Costs of Shuffle Join:\n",
    "\n",
    "```text\n",
    "Network Cost:\n",
    "- Both tables transferred across network\n",
    "- Example: 50 GB + 2 GB = 52 GB transfer\n",
    "\n",
    "Disk I/O Cost:\n",
    "- Write shuffle files to disk\n",
    "- Read shuffle files from disk\n",
    "- Example: 52 GB write + 52 GB read = 104 GB I/O\n",
    "\n",
    "Memory Cost:\n",
    "- Buffer data during shuffle\n",
    "- Sort data in memory\n",
    "\n",
    "Time Cost:\n",
    "- Network transfer time\n",
    "- Disk I/O time\n",
    "- Sorting time\n",
    "- Example: Can take 10-30 minutes for large tables\n",
    "```\n",
    "\n",
    "### When Shuffle Join Happens:\n",
    "\n",
    "- Both tables are large (> 10 MB)\n",
    "- No broadcast hint provided\n",
    "- Default join strategy\n",
    "\n",
    "---\n",
    "\n",
    "## Broadcast Join (Map-Side Join)\n",
    "\n",
    "### What is Broadcast Join?\n",
    "\n",
    "Broadcast Join sends the entire small table to every executor, eliminating the need to shuffle the large table.\n",
    "\n",
    "### Visual Explanation:\n",
    "\n",
    "```text\n",
    "BROADCAST STRATEGY:\n",
    "===================\n",
    "\n",
    "Table A (Countries - Small)    Table B (Customers - Large)\n",
    "195 rows, 10 KB               100M rows, 20 GB\n",
    "\n",
    "Step 1: Broadcast Table A\n",
    "    Send 10 KB to ALL executors\n",
    "    Each executor gets a complete copy\n",
    "\n",
    "Step 2: No shuffle needed!\n",
    "    Large table stays in place\n",
    "    \n",
    "Executor 1:                   \n",
    "  Memory: Countries (10 KB) <- Broadcasted\n",
    "  Processes: Partition 1 of Customers\n",
    "  Joins locally (fast!)\n",
    "\n",
    "Executor 2:\n",
    "  Memory: Countries (10 KB) <- Broadcasted\n",
    "  Processes: Partition 2 of Customers\n",
    "  Joins locally (fast!)\n",
    "\n",
    "Executor 3:\n",
    "  Memory: Countries (10 KB) <- Broadcasted\n",
    "  Processes: Partition 3 of Customers\n",
    "  Joins locally (fast!)\n",
    "\n",
    "Total Network Transfer: 10 KB x 10 executors = 100 KB\n",
    "vs Shuffle Join would transfer: 20 GB!\n",
    "```\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "```text\n",
    "Step 1: Collect small table\n",
    "    Driver collects entire small table\n",
    "\n",
    "Step 2: Broadcast to executors\n",
    "    Send small table to all executors\n",
    "    Each executor stores it in memory\n",
    "\n",
    "Step 3: Hash join\n",
    "    Each executor joins its partition with broadcasted table\n",
    "    No shuffle needed!\n",
    "\n",
    "Step 4: Return results\n",
    "```\n",
    "\n",
    "### Code Example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Small table\n",
    "countries = spark.read.parquet(\"/countries\")  # 195 rows, 10 KB\n",
    "\n",
    "# Large table\n",
    "customers = spark.read.parquet(\"/customers\")  # 100M rows, 20 GB\n",
    "\n",
    "# Broadcast Join (explicit)\n",
    "result = customers.join(broadcast(countries), \"country_code\")\n",
    "\n",
    "# What Spark does internally:\n",
    "# 1. Send 10 KB to all executors (minimal network)\n",
    "# 2. No shuffle of 20 GB customers table!\n",
    "# 3. Join happens locally on each executor\n",
    "# 4. Total network: ~100 KB vs 20 GB!\n",
    "```\n",
    "\n",
    "### Benefits of Broadcast Join:\n",
    "\n",
    "```text\n",
    "Network Cost: Minimal\n",
    "- Only small table transferred\n",
    "- Example: 10 KB x 10 executors = 100 KB\n",
    "\n",
    "Disk I/O Cost: Zero\n",
    "- No shuffle files written\n",
    "- No shuffle files read\n",
    "\n",
    "Memory Cost: Low\n",
    "- Small table in executor memory\n",
    "- Example: 10 KB per executor\n",
    "\n",
    "Time Cost: Very fast\n",
    "- No shuffle overhead\n",
    "- Example: Can be 10-100x faster!\n",
    "```\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Small table must fit in executor memory\n",
    "- Default broadcast limit: 10 MB\n",
    "- Can increase but be careful with memory\n",
    "\n",
    "---\n",
    "\n",
    "## Shuffle Join vs Broadcast Join Comparison\n",
    "\n",
    "| Aspect | Shuffle Join | Broadcast Join |\n",
    "|--------|--------------|----------------|\n",
    "| Network Transfer | Both tables | Only small table |\n",
    "| Disk I/O | High (shuffle files) | Zero |\n",
    "| Speed | Slow | Fast |\n",
    "| Memory Usage | Moderate | Low (per executor) |\n",
    "| Use Case | Both tables large | One table small |\n",
    "| Cost | High | Low |\n",
    "| Typical Time | 10-30 minutes | 1-5 minutes |\n",
    "\n",
    "---\n",
    "\n",
    "## Real Production Examples\n",
    "\n",
    "### Example 1: E-Commerce Order Analysis\n",
    "\n",
    "**Scenario:** Join 100 million orders with 50 stores\n",
    "\n",
    "```python\n",
    "# Tables\n",
    "stores = spark.read.parquet(\"/stores\")      # 50 rows, 2 KB\n",
    "orders = spark.read.parquet(\"/orders\")      # 100M rows, 50 GB\n",
    "\n",
    "# âŒ BAD: Shuffle Join (default if you forget broadcast)\n",
    "result = orders.join(stores, \"store_id\")\n",
    "# Cost: Shuffles 50 GB\n",
    "# Time: 15 minutes\n",
    "# Money: $200 per run\n",
    "\n",
    "# âœ… GOOD: Broadcast Join\n",
    "result = orders.join(broadcast(stores), \"store_id\")\n",
    "# Cost: Broadcasts 2 KB\n",
    "# Time: 2 minutes  \n",
    "# Money: $20 per run\n",
    "# Savings: $180 per run!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Banking Fraud Detection\n",
    "\n",
    "**Scenario:** Check 500M transactions against 1000 fraud rules\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Tables\n",
    "transactions = spark.read.parquet(\"/transactions\")  # 500M rows, 100 GB\n",
    "fraud_rules = spark.read.parquet(\"/fraud_rules\")    # 1000 rows, 50 KB\n",
    "categories = spark.read.parquet(\"/categories\")      # 500 rows, 20 KB\n",
    "\n",
    "# âŒ WRONG: Multiple shuffle joins\n",
    "result = transactions.join(fraud_rules, \"rule_id\") \\\n",
    "                    .join(categories, \"category_id\")\n",
    "# Problem: Shuffles 100 GB twice!\n",
    "# Time: 45 minutes\n",
    "# Often fails with OOM errors\n",
    "\n",
    "# âœ… CORRECT: Broadcast both small tables\n",
    "result = transactions.join(broadcast(fraud_rules), \"rule_id\") \\\n",
    "                    .join(broadcast(categories), \"category_id\")\n",
    "# Network: Only 70 KB broadcasted\n",
    "# Time: 5 minutes\n",
    "# Never fails!\n",
    "# Savings: 9x faster!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Ad Tech - Real-Time Bidding\n",
    "\n",
    "**Scenario:** Process 10 billion ad impressions with dimension tables\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Fact table (huge)\n",
    "impressions = spark.read.parquet(\"/impressions\")    # 10B rows, 5 TB\n",
    "\n",
    "# Dimension tables (small)\n",
    "advertisers = spark.read.parquet(\"/advertisers\")    # 50K rows, 5 MB\n",
    "campaigns = spark.read.parquet(\"/campaigns\")        # 200K rows, 20 MB\n",
    "geo = spark.read.parquet(\"/geo_locations\")          # 10K rows, 500 KB\n",
    "\n",
    "# âŒ DISASTER: Shuffle joins\n",
    "result = impressions.join(advertisers, \"advertiser_id\") \\\n",
    "                   .join(campaigns, \"campaign_id\") \\\n",
    "                   .join(geo, \"geo_id\")\n",
    "# Would shuffle 5 TB multiple times!\n",
    "# Time: 8 hours\n",
    "# Cost: $2000 per run\n",
    "# Likely to fail!\n",
    "\n",
    "# âœ… PRODUCTION-READY: Broadcast all dimensions\n",
    "result = impressions.join(broadcast(advertisers), \"advertiser_id\") \\\n",
    "                   .join(broadcast(campaigns), \"campaign_id\") \\\n",
    "                   .join(broadcast(geo), \"geo_id\")\n",
    "# Broadcasts only 25.5 MB total\n",
    "# Time: 30 minutes\n",
    "# Cost: $120 per run\n",
    "# Monthly savings: $56,400!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Which Join?\n",
    "\n",
    "### Use Broadcast Join When:\n",
    "\n",
    "1. One table is small (< 100 MB)\n",
    "2. Joining fact table with dimension tables\n",
    "3. You have sufficient executor memory\n",
    "4. Performance is critical\n",
    "\n",
    "**Common Broadcast Candidates:**\n",
    "- Countries (195 rows)\n",
    "- US States (50 rows)\n",
    "- Product categories (< 10K rows)\n",
    "- Status codes (< 100 rows)\n",
    "- Currency codes (180 rows)\n",
    "- Date dimensions (< 10K rows)\n",
    "- Configuration tables\n",
    "- Lookup tables\n",
    "\n",
    "---\n",
    "\n",
    "### Use Shuffle Join When:\n",
    "\n",
    "1. Both tables are large (> 1 GB each)\n",
    "2. Cannot fit small table in memory\n",
    "3. No dimension tables involved\n",
    "4. Fact-to-fact table joins\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Both tables are huge - no choice but to shuffle\n",
    "sales_2023 = spark.read.parquet(\"/sales/2023\")  # 500 GB\n",
    "sales_2024 = spark.read.parquet(\"/sales/2024\")  # 600 GB\n",
    "\n",
    "# Shuffle join (unavoidable)\n",
    "result = sales_2023.join(sales_2024, \"product_id\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration and Tuning\n",
    "\n",
    "### Broadcast Threshold\n",
    "\n",
    "```python\n",
    "# Default broadcast threshold: 10 MB\n",
    "# Check current value\n",
    "current = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Current threshold: {current} bytes\")\n",
    "\n",
    "# Increase to 100 MB (if you have memory)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)\n",
    "\n",
    "# Disable auto broadcast (force manual control)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "```\n",
    "\n",
    "### Broadcast Timeout\n",
    "\n",
    "```python\n",
    "# Default: 300 seconds (5 minutes)\n",
    "# Increase if broadcasting large tables\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 600)  # 10 minutes\n",
    "```\n",
    "\n",
    "### Check Table Size Before Joining\n",
    "\n",
    "```python\n",
    "def should_broadcast(df, max_size_mb=100):\n",
    "    \"\"\"\n",
    "    Check if DataFrame should be broadcasted\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check\n",
    "        max_size_mb: Maximum size in MB for broadcasting\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if table should be broadcasted\n",
    "    \"\"\"\n",
    "    # Cache and count to get accurate size\n",
    "    df.cache()\n",
    "    row_count = df.count()\n",
    "    \n",
    "    # Get size from Spark's statistics\n",
    "    size_bytes = df._jdf.queryExecution().optimizedPlan().stats().sizeInBytes()\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Table size: {size_mb:.2f} MB ({row_count:,} rows)\")\n",
    "    \n",
    "    if size_mb < max_size_mb:\n",
    "        print(f\"âœ… Recommend BROADCAST join\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ Use SHUFFLE join\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "if should_broadcast(countries, 100):\n",
    "    result = orders.join(broadcast(countries), \"country_id\")\n",
    "else:\n",
    "    result = orders.join(countries, \"country_id\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Verify Join Type\n",
    "\n",
    "### Method 1: Check Execution Plan\n",
    "\n",
    "```python\n",
    "# Create join\n",
    "result = orders.join(broadcast(countries), \"country_id\")\n",
    "\n",
    "# Check plan\n",
    "result.explain()\n",
    "\n",
    "# Look for:\n",
    "# âœ… \"BroadcastHashJoin\" = Broadcast join (good!)\n",
    "# âŒ \"SortMergeJoin\" = Shuffle join\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "```text\n",
    "== Physical Plan ==\n",
    "BroadcastHashJoin [country_id#123], [country_id#456]\n",
    ":- LocalTableScan [country_id#123, name#124]\n",
    "+- *(1) FileScan parquet [order_id#789, country_id#456]\n",
    "\n",
    "This shows BroadcastHashJoin - Perfect!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Method 2: Spark UI\n",
    "\n",
    "```text\n",
    "Steps to check in Spark UI:\n",
    "1. Open Spark UI (http://localhost:4040 or your cluster UI)\n",
    "2. Click \"SQL\" tab\n",
    "3. Click on your query\n",
    "4. Look at the DAG visualization:\n",
    "   \n",
    "   âœ… Broadcast join shows:\n",
    "      - \"BroadcastExchange\" node\n",
    "      - No \"Exchange\" (shuffle) on large table\n",
    "   \n",
    "   âŒ Shuffle join shows:\n",
    "      - \"Exchange\" nodes on both tables\n",
    "      - \"Sort\" operations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Method 3: Check Metrics\n",
    "\n",
    "```python\n",
    "# In Spark UI, go to Stages tab\n",
    "# Check stage metrics:\n",
    "\n",
    "# âœ… Broadcast Join:\n",
    "#    Shuffle Write: 0 B\n",
    "#    Shuffle Read: 0 B\n",
    "\n",
    "# âŒ Shuffle Join:\n",
    "#    Shuffle Write: 50 GB\n",
    "#    Shuffle Read: 50 GB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "### Practice 1: Always Broadcast Dimension Tables\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# âœ… ALWAYS do this for dimension tables\n",
    "result = fact_table.join(broadcast(dim_table), \"key\")\n",
    "\n",
    "# Common dimensions to broadcast:\n",
    "countries = broadcast(spark.read.parquet(\"/countries\"))\n",
    "categories = broadcast(spark.read.parquet(\"/categories\"))\n",
    "products = broadcast(spark.read.parquet(\"/products\"))\n",
    "statuses = broadcast(spark.read.parquet(\"/statuses\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Practice 2: Order Your Joins\n",
    "\n",
    "```python\n",
    "# When joining multiple tables, broadcast small ones first\n",
    "\n",
    "# âœ… GOOD ORDER:\n",
    "result = large_fact \\\n",
    "    .join(broadcast(small_dim1), \"key1\") \\\n",
    "    .join(broadcast(small_dim2), \"key2\") \\\n",
    "    .join(medium_table, \"key3\")  # Only this shuffles\n",
    "\n",
    "# âŒ BAD ORDER:\n",
    "result = large_fact \\\n",
    "    .join(medium_table, \"key3\") \\  # Shuffles here\n",
    "    .join(broadcast(small_dim1), \"key1\") \\  # Already shuffled!\n",
    "    .join(broadcast(small_dim2), \"key2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Practice 3: Cache Broadcasted Tables\n",
    "\n",
    "```python\n",
    "# If using same small table multiple times, cache it\n",
    "\n",
    "# Load and cache dimension\n",
    "countries = spark.read.parquet(\"/countries\").cache()\n",
    "countries.count()  # Materialize cache\n",
    "\n",
    "# Use in multiple joins\n",
    "result1 = orders.join(broadcast(countries), \"country_id\")\n",
    "result2 = customers.join(broadcast(countries), \"country_id\")\n",
    "result3 = stores.join(broadcast(countries), \"country_id\")\n",
    "\n",
    "# All three joins use cached version!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Practice 4: Monitor and Alert\n",
    "\n",
    "```python\n",
    "# Set up monitoring for shuffle operations\n",
    "\n",
    "def check_shuffle_size(df, threshold_gb=10):\n",
    "    \"\"\"Alert if shuffle size exceeds threshold\"\"\"\n",
    "    \n",
    "    # Get shuffle write metrics from last stage\n",
    "    # (In production, integrate with monitoring system)\n",
    "    \n",
    "    plan = df.explain(mode=\"formatted\")\n",
    "    \n",
    "    if \"Exchange\" in str(plan):\n",
    "        print(\"âš ï¸ WARNING: Shuffle detected!\")\n",
    "        print(\"Consider using broadcast join if possible\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âœ… No shuffle - optimized join\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "result = orders.join(countries, \"country_id\")\n",
    "check_shuffle_size(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Production Mistakes\n",
    "\n",
    "### Mistake 1: Broadcasting Too Large Tables\n",
    "\n",
    "```python\n",
    "# âŒ WRONG: Broadcasting 2 GB table\n",
    "huge_dim = spark.read.parquet(\"/huge_dimension\")  # 2 GB!\n",
    "result = facts.join(broadcast(huge_dim), \"key\")\n",
    "\n",
    "# What happens:\n",
    "# - Each executor gets 2 GB in memory\n",
    "# - 50 executors x 2 GB = 100 GB total memory!\n",
    "# - Out of memory errors\n",
    "# - Job fails\n",
    "\n",
    "# âœ… CORRECT: Check size first\n",
    "if should_broadcast(huge_dim, max_size_mb=100):\n",
    "    result = facts.join(broadcast(huge_dim), \"key\")\n",
    "else:\n",
    "    result = facts.join(huge_dim, \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Forgetting to Broadcast\n",
    "\n",
    "```python\n",
    "# âŒ COMMON MISTAKE: Small table not broadcasted\n",
    "products = spark.read.parquet(\"/products\")  # Only 5 MB!\n",
    "sales = spark.read.parquet(\"/sales\")        # 1 TB\n",
    "\n",
    "result = sales.join(products, \"product_id\")  # Forgot broadcast()!\n",
    "\n",
    "# Impact:\n",
    "# - Shuffles 1 TB of sales data unnecessarily\n",
    "# - Takes 2 hours instead of 5 minutes\n",
    "# - Costs $800 instead of $40\n",
    "\n",
    "# âœ… FIX: Add broadcast()\n",
    "result = sales.join(broadcast(products), \"product_id\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Multiple Unnecessary Shuffles\n",
    "\n",
    "```python\n",
    "# âŒ WRONG: Multiple shuffle joins\n",
    "customers = spark.read.parquet(\"/customers\")  # 50 GB\n",
    "orders = spark.read.parquet(\"/orders\")        # 100 GB\n",
    "products = spark.read.parquet(\"/products\")    # 200 MB\n",
    "countries = spark.read.parquet(\"/countries\")  # 10 KB\n",
    "\n",
    "result = customers.join(orders, \"customer_id\") \\      # Shuffle 1\n",
    "                 .join(products, \"product_id\") \\      # Shuffle 2\n",
    "                 .join(countries, \"country_id\")       # Shuffle 3\n",
    "\n",
    "# Impact: 150 GB shuffled 3 times!\n",
    "\n",
    "# âœ… CORRECT: Broadcast small tables\n",
    "result = customers.join(orders, \"customer_id\") \\           # Shuffle (unavoidable)\n",
    "                 .join(broadcast(products), \"product_id\") \\ # No shuffle!\n",
    "                 .join(broadcast(countries), \"country_id\")  # No shuffle!\n",
    "\n",
    "# Impact: Only ONE shuffle!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "```text\n",
    "Need to join two tables?\n",
    "    |\n",
    "    v\n",
    "Is one table < 10 MB?\n",
    "    |\n",
    "    +-- YES --> âœ… Use BROADCAST JOIN (automatic)\n",
    "    |           result = large.join(broadcast(small), \"key\")\n",
    "    |\n",
    "    +-- NO --> Is one table < 100 MB?\n",
    "               |\n",
    "               +-- YES --> Do you have enough memory?\n",
    "               |           |\n",
    "               |           +-- YES --> âœ… Use BROADCAST JOIN (manual)\n",
    "               |           |           spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100*1024*1024)\n",
    "               |           |           result = large.join(broadcast(small), \"key\")\n",
    "               |           |\n",
    "               |           +-- NO --> âŒ Use SHUFFLE JOIN\n",
    "               |                      result = table1.join(table2, \"key\")\n",
    "               |\n",
    "               +-- NO --> Are BOTH tables > 10 GB?\n",
    "                          |\n",
    "                          +-- YES --> âŒ Use SHUFFLE JOIN (no choice)\n",
    "                          |           result = table1.join(table2, \"key\")\n",
    "                          |\n",
    "                          +-- NO --> Test both strategies\n",
    "                                     Pick the faster one\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "### Real Numbers from Production\n",
    "\n",
    "**Scenario:** Join 100M orders with 50 stores\n",
    "\n",
    "| Metric | Shuffle Join | Broadcast Join | Improvement |\n",
    "|--------|--------------|----------------|-------------|\n",
    "| Network Transfer | 50 GB | 2 KB | 25,000,000x |\n",
    "| Disk I/O | 100 GB | 0 GB | Infinite |\n",
    "| Execution Time | 15 min | 2 min | 7.5x faster |\n",
    "| Cost per run | $200 | $20 | $180 saved |\n",
    "| Monthly cost | $6,000 | $600 | $5,400 saved |\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Working Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col, sum\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BroadcastJoinExample\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dimension tables (small - always broadcast these)\n",
    "countries = spark.read.parquet(\"/data/countries\")  # 195 rows\n",
    "categories = spark.read.parquet(\"/data/categories\")  # 1000 rows\n",
    "products = spark.read.parquet(\"/data/products\")  # 50000 rows, 10 MB\n",
    "\n",
    "# Cache small tables if used multiple times\n",
    "countries.cache()\n",
    "categories.cache()\n",
    "products.cache()\n",
    "\n",
    "# Materialize cache\n",
    "countries.count()\n",
    "categories.count()\n",
    "products.count()\n",
    "\n",
    "# Load fact tables (large)\n",
    "orders = spark.read.parquet(\"/data/orders\")  # 100M rows, 50 GB\n",
    "customers = spark.read.parquet(\"/data/customers\")  # 10M rows, 5 GB\n",
    "\n",
    "# Perform joins with broadcast\n",
    "result = orders \\\n",
    "    .join(broadcast(products), \"product_id\") \\\n",
    "    .join(broadcast(categories), \"category_id\") \\\n",
    "    .join(broadcast(countries), \"country_id\") \\\n",
    "    .join(customers, \"customer_id\")  # Only this shuffles\n",
    "\n",
    "# Aggregate\n",
    "final_result = result.groupBy(\"country_id\", \"category_id\") \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "\n",
    "# Check execution plan\n",
    "print(\"Execution Plan:\")\n",
    "final_result.explain()\n",
    "\n",
    "# Execute and save\n",
    "final_result.write.mode(\"overwrite\").parquet(\"/output/sales_summary\")\n",
    "\n",
    "print(\"âœ… Job completed successfully!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Broadcast Join** is for small tables (< 100 MB)\n",
    "   - Fast, no shuffle, minimal network\n",
    "   - Use for dimension tables\n",
    "\n",
    "2. **Shuffle Join** is for large tables (> 1 GB)\n",
    "   - Slow, requires shuffle, lots of network\n",
    "   - Use when both tables are huge\n",
    "\n",
    "3. **Always broadcast dimension tables**\n",
    "   - Countries, categories, statuses, etc.\n",
    "   - Can save 10-100x in performance\n",
    "\n",
    "4. **Check execution plans**\n",
    "   - Look for \"BroadcastHashJoin\" vs \"SortMergeJoin\"\n",
    "   - Monitor in Spark UI\n",
    "\n",
    "5. **Production impact is HUGE**\n",
    "   - Can save thousands of dollars per month\n",
    "   - Can reduce execution time by 10x\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# âœ… CORRECT PATTERNS\n",
    "\n",
    "# Pattern 1: Explicit broadcast\n",
    "result = large_table.join(broadcast(small_table), \"key\")\n",
    "\n",
    "# Pattern 2: Multiple broadcasts\n",
    "result = large_table \\\n",
    "    .join(broadcast(dim1), \"key1\") \\\n",
    "    .join(broadcast(dim2), \"key2\") \\\n",
    "    .join(broadcast(dim3), \"key3\")\n",
    "\n",
    "# Pattern 3: Cache + broadcast\n",
    "small_table.cache().count()\n",
    "result = large_table.join(broadcast(small_table), \"key\")\n",
    "\n",
    "# Pattern 4: Configure threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)\n",
    "\n",
    "# âŒ WRONG PATTERNS\n",
    "\n",
    "# Wrong 1: Forget broadcast\n",
    "result = large_table.join(small_table, \"key\")  # Shuffles unnecessarily!\n",
    "\n",
    "# Wrong 2: Broadcast huge table\n",
    "result = table1.join(broadcast(huge_table), \"key\")  # OOM errors!\n",
    "\n",
    "# Wrong 3: Multiple shuffles\n",
    "result = t1.join(t2, \"k1\").join(t3, \"k2\").join(t4, \"k3\")  # All shuffle!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Broadcast small, shuffle large. Your wallet will thank you! ðŸ’°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6362f83-ee95-427f-b86a-1eb96e67570d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark SQL Engine & Catalyst Optimizer\n",
    "\n",
    "## What is Spark SQL Engine?\n",
    "\n",
    "Spark SQL Engine is the component that processes DataFrame and SQL queries in Spark. It optimizes your queries before execution to make them faster and more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## The Catalyst Optimizer Flow\n",
    "\n",
    "```text\n",
    "User Query\n",
    "    |\n",
    "    v\n",
    "Unresolved Logical Plan\n",
    "    |\n",
    "    v\n",
    "[Analysis Phase]\n",
    "    |\n",
    "    v\n",
    "Resolved Logical Plan\n",
    "    |\n",
    "    v\n",
    "[Optimization Phase]\n",
    "    |\n",
    "    v\n",
    "Optimized Logical Plan\n",
    "    |\n",
    "    v\n",
    "[Code Generation]\n",
    "    |\n",
    "    v\n",
    "Physical Plans\n",
    "    |\n",
    "    v\n",
    "[Cost Model Selection]\n",
    "    |\n",
    "    v\n",
    "Best Physical Plan\n",
    "    |\n",
    "    v\n",
    "Executors (Execution)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Unresolved Logical Plan\n",
    "\n",
    "**What happens:**\n",
    "- Spark parses your SQL or DataFrame code\n",
    "- Creates initial logical plan\n",
    "- Column names and table names are NOT verified yet\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "df.select(\"name\", \"age\").filter(col(\"age\") > 30)\n",
    "```\n",
    "\n",
    "**Unresolved Plan:**\n",
    "```text\n",
    "Filter (age > 30)\n",
    "  |\n",
    "  v\n",
    "Project (name, age)\n",
    "  |\n",
    "  v\n",
    "Scan (table)\n",
    "```\n",
    "\n",
    "**Status:** Column names might be wrong, table might not exist - not checked yet!\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: Analysis (Using Catalog)\n",
    "\n",
    "**What happens:**\n",
    "- Spark checks the **Catalog** (metadata store)\n",
    "- Verifies tables exist\n",
    "- Verifies columns exist\n",
    "- Resolves data types\n",
    "- Checks if query is valid\n",
    "\n",
    "**Catalog contains:**\n",
    "- Table schemas\n",
    "- Column names and types\n",
    "- Table locations\n",
    "- Partitioning information\n",
    "- Statistics\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# If \"age\" column doesn't exist, Analysis phase will throw error:\n",
    "# AnalysisException: Column 'age' does not exist\n",
    "```\n",
    "\n",
    "**After Analysis:**\n",
    "```text\n",
    "Resolved Logical Plan\n",
    "- All tables verified âœ“\n",
    "- All columns verified âœ“\n",
    "- Data types resolved âœ“\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 3: Optimized Logical Plan\n",
    "\n",
    "**What happens:**\n",
    "- Catalyst Optimizer applies optimization rules\n",
    "- Improves the query plan\n",
    "- Makes query faster without changing results\n",
    "\n",
    "**Common Optimizations:**\n",
    "\n",
    "### 1. Predicate Pushdown\n",
    "```python\n",
    "# Your code\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df = df.select(\"name\", \"age\", \"salary\")\n",
    "df = df.filter(col(\"age\") > 30)\n",
    "\n",
    "# Spark optimizes to:\n",
    "# Read parquet with filter age > 30 (filter happens at source!)\n",
    "```\n",
    "\n",
    "### 2. Column Pruning\n",
    "```python\n",
    "# Your code\n",
    "df = spark.read.parquet(\"/data\")  # Table has 50 columns\n",
    "df = df.select(\"name\", \"age\")     # Only need 2 columns\n",
    "\n",
    "# Spark optimizes to:\n",
    "# Read ONLY name and age columns (doesn't read all 50!)\n",
    "```\n",
    "\n",
    "### 3. Constant Folding\n",
    "```python\n",
    "# Your code\n",
    "df = df.filter((col(\"age\") + 10 - 10) > 30)\n",
    "\n",
    "# Spark optimizes to:\n",
    "# df.filter(col(\"age\") > 30)  # Simplified!\n",
    "```\n",
    "\n",
    "### 4. Filter Reordering\n",
    "```python\n",
    "# Your code\n",
    "df = df.filter(col(\"country\") == \"USA\")  # Keeps 10% of data\n",
    "df = df.filter(col(\"age\") > 30)          # Keeps 50% of data\n",
    "\n",
    "# Spark may reorder to:\n",
    "# Filter country first (removes 90% early!)\n",
    "# Then filter age on remaining 10%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 4: Physical Plans\n",
    "\n",
    "**What happens:**\n",
    "- Catalyst generates MULTIPLE physical execution plans\n",
    "- Each plan is a different way to execute the query\n",
    "\n",
    "**Example Physical Plans:**\n",
    "\n",
    "**Plan 1:** Broadcast Join\n",
    "```text\n",
    "BroadcastHashJoin\n",
    "  - Broadcast small table\n",
    "  - Join with large table\n",
    "Cost: Low (if small table fits in memory)\n",
    "```\n",
    "\n",
    "**Plan 2:** Shuffle Join\n",
    "```text\n",
    "SortMergeJoin\n",
    "  - Shuffle both tables\n",
    "  - Sort and merge\n",
    "Cost: High (shuffle overhead)\n",
    "```\n",
    "\n",
    "**Plan 3:** Different partition strategy\n",
    "```text\n",
    "Use different number of partitions\n",
    "Cost: Varies\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 5: Cost Model\n",
    "\n",
    "**What happens:**\n",
    "- Spark evaluates the **cost** of each physical plan\n",
    "- Cost based on:\n",
    "  - Data size\n",
    "  - Number of partitions\n",
    "  - Shuffle operations\n",
    "  - Memory usage\n",
    "  - CPU usage\n",
    "\n",
    "**Cost Model considers:**\n",
    "```text\n",
    "- How much data to shuffle?\n",
    "- How much memory needed?\n",
    "- How many disk I/O operations?\n",
    "- Network bandwidth required?\n",
    "- Number of tasks to create?\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "Plan 1 (Broadcast Join):\n",
    "  - Shuffle: 0 GB\n",
    "  - Memory: 10 MB\n",
    "  - Cost Score: 100\n",
    "\n",
    "Plan 2 (Shuffle Join):\n",
    "  - Shuffle: 50 GB\n",
    "  - Memory: 5 GB\n",
    "  - Cost Score: 10000\n",
    "\n",
    "Winner: Plan 1 (lowest cost!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 6: Best Physical Plan Selection\n",
    "\n",
    "**What happens:**\n",
    "- Spark picks the plan with LOWEST cost\n",
    "- This becomes the execution plan\n",
    "- Sent to executors for actual execution\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 7: Execution on Executors\n",
    "\n",
    "**What happens:**\n",
    "- Best physical plan is executed\n",
    "- Distributed across executors\n",
    "- Tasks run in parallel\n",
    "- Results collected\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Example\n",
    "\n",
    "```python\n",
    "# User writes query\n",
    "df = spark.read.parquet(\"/sales\")\n",
    "df = df.filter(col(\"year\") == 2024)\n",
    "df = df.filter(col(\"country\") == \"USA\")\n",
    "df = df.select(\"product\", \"revenue\")\n",
    "df = df.groupBy(\"product\").sum(\"revenue\")\n",
    "result = df.show()\n",
    "```\n",
    "\n",
    "### Step-by-Step through Catalyst:\n",
    "\n",
    "**1. Unresolved Logical Plan:**\n",
    "```text\n",
    "Aggregate (product, sum(revenue))\n",
    "  |\n",
    "Project (product, revenue)\n",
    "  |\n",
    "Filter (country = USA)\n",
    "  |\n",
    "Filter (year = 2024)\n",
    "  |\n",
    "Scan (/sales)\n",
    "```\n",
    "\n",
    "**2. Analysis (check Catalog):**\n",
    "```text\n",
    "âœ“ Table /sales exists\n",
    "âœ“ Columns: product, revenue, country, year exist\n",
    "âœ“ Data types match\n",
    "Result: Resolved Logical Plan\n",
    "```\n",
    "\n",
    "**3. Optimization:**\n",
    "```text\n",
    "Optimized plan:\n",
    "- Push filters to parquet read\n",
    "- Read only: product, revenue, year, country\n",
    "- Filter at source: year=2024 AND country=USA\n",
    "- Then aggregate\n",
    "\n",
    "Result: Read 1GB instead of 100GB!\n",
    "```\n",
    "\n",
    "**4. Physical Plans generated:**\n",
    "```text\n",
    "Plan A: Hash aggregation, 200 partitions\n",
    "Plan B: Sort-based aggregation, 100 partitions\n",
    "Plan C: Partial aggregation, 200 partitions\n",
    "```\n",
    "\n",
    "**5. Cost Model evaluates:**\n",
    "```text\n",
    "Plan A cost: 1000\n",
    "Plan B cost: 1500\n",
    "Plan C cost: 800 <- WINNER!\n",
    "```\n",
    "\n",
    "**6. Execute Plan C on executors**\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters in Production\n",
    "\n",
    "### Without Catalyst Optimizer:\n",
    "```python\n",
    "# Naive execution\n",
    "df = spark.read.parquet(\"/data\")  # Reads 100GB, all 50 columns\n",
    "df = df.filter(col(\"age\") > 30)   # Filters in Spark\n",
    "df = df.select(\"name\", \"age\")     # Drops columns after reading\n",
    "\n",
    "Time: 30 minutes\n",
    "Data read: 100 GB\n",
    "```\n",
    "\n",
    "### With Catalyst Optimizer:\n",
    "```python\n",
    "# Same code, but Catalyst optimizes\n",
    "df = spark.read.parquet(\"/data\")  # Reads only name, age columns with filter\n",
    "df = df.filter(col(\"age\") > 30)   # Filter pushed to parquet\n",
    "df = df.select(\"name\", \"age\")     # Already optimized\n",
    "\n",
    "Time: 3 minutes (10x faster!)\n",
    "Data read: 5 GB (95% less!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components Summary\n",
    "\n",
    "| Component | Role |\n",
    "|-----------|------|\n",
    "| Unresolved Logical Plan | Initial parse of query |\n",
    "| Catalog | Metadata store (tables, columns, types) |\n",
    "| Analysis | Verify query is valid |\n",
    "| Optimizer | Apply optimization rules |\n",
    "| Physical Plans | Different execution strategies |\n",
    "| Cost Model | Evaluate each plan's cost |\n",
    "| Best Plan | Lowest cost plan selected |\n",
    "| Executors | Execute the plan |\n",
    "\n",
    "---\n",
    "\n",
    "## Common Optimizations Applied\n",
    "\n",
    "1. **Predicate Pushdown** - Push filters to data source\n",
    "2. **Column Pruning** - Read only needed columns\n",
    "3. **Constant Folding** - Simplify expressions\n",
    "4. **Join Reordering** - Optimize join sequence\n",
    "5. **Broadcast Join** - Broadcast small tables\n",
    "6. **Partition Pruning** - Skip irrelevant partitions\n",
    "\n",
    "---\n",
    "\n",
    "## How to See the Plan\n",
    "\n",
    "```python\n",
    "# See all phases\n",
    "df.explain(True)\n",
    "\n",
    "# Output shows:\n",
    "# == Parsed Logical Plan ==\n",
    "# == Analyzed Logical Plan ==\n",
    "# == Optimized Logical Plan ==\n",
    "# == Physical Plan ==\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "```text\n",
    "== Optimized Logical Plan ==\n",
    "Aggregate [product], [sum(revenue)]\n",
    "  +- Project [product, revenue]\n",
    "      +- Filter (year = 2024 AND country = USA)\n",
    "          +- Relation [product, revenue, year, country]\n",
    "          \n",
    "== Physical Plan ==\n",
    "HashAggregate\n",
    "  +- Exchange hashpartitioning(product)\n",
    "      +- HashAggregate\n",
    "          +- FileScan parquet [product, revenue, year, country]\n",
    "              PushedFilters: [year=2024, country=USA]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Production Tips\n",
    "\n",
    "### Tip 1: Trust the Optimizer\n",
    "```python\n",
    "# Don't over-optimize manually\n",
    "# Catalyst is very smart!\n",
    "\n",
    "# âŒ Don't do this:\n",
    "df = df.filter(col(\"age\") > 30)\n",
    "df = df.select(\"name\", \"age\")\n",
    "df = df.filter(col(\"name\").isNotNull())  # Trying to optimize\n",
    "\n",
    "# âœ… Do this:\n",
    "df = df.filter((col(\"age\") > 30) & col(\"name\").isNotNull())\n",
    "df = df.select(\"name\", \"age\")\n",
    "# Catalyst will optimize it anyway!\n",
    "```\n",
    "\n",
    "### Tip 2: Check Execution Plans\n",
    "```python\n",
    "# Always check plans for expensive queries\n",
    "df.explain()\n",
    "\n",
    "# Look for:\n",
    "# âœ“ PushedFilters (good!)\n",
    "# âœ“ BroadcastHashJoin (good for small tables!)\n",
    "# âœ— CartesianProduct (very bad!)\n",
    "# âœ— No pushed filters (might be bad)\n",
    "```\n",
    "\n",
    "### Tip 3: Collect Statistics\n",
    "```python\n",
    "# Help Catalyst make better decisions\n",
    "spark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS\")\n",
    "\n",
    "# This helps Cost Model choose better plans\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Catalyst Optimizer automatically:**\n",
    "- Validates your query (Analysis)\n",
    "- Optimizes execution plan (Optimization)\n",
    "- Generates multiple strategies (Physical Plans)\n",
    "- Picks the fastest one (Cost Model)\n",
    "- Executes efficiently (Executors)\n",
    "\n",
    "**Result:** Your queries run 10-100x faster without you doing anything!\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```text\n",
    "Query Flow:\n",
    "User Code -> Unresolved Plan -> Analysis -> Resolved Plan \n",
    "-> Optimization -> Optimized Plan -> Physical Plans \n",
    "-> Cost Model -> Best Plan -> Executors -> Results\n",
    "\n",
    "Key Insight:\n",
    "Spark doesn't execute what you write.\n",
    "It executes the OPTIMIZED version of what you write!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Catalyst Optimizer is working behind the scenes to make your queries faster. Trust it, but verify with explain()! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b63610-6fed-44d3-b6ab-8756752b996e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Memory Management & Advanced Concepts\n",
    "\n",
    "## Driver Memory Management\n",
    "\n",
    "### Two Main Sections:\n",
    "\n",
    "**1. JVM Heap Memory**\n",
    "- Core memory used by Driver's JVM\n",
    "- Stores DAG, metadata, broadcast variables, task scheduling info\n",
    "\n",
    "**2. Overhead Memory**\n",
    "- Non-heap memory for JVM threads, shared libraries, native code\n",
    "\n",
    "```python\n",
    "# Configure driver memory\n",
    "spark-submit \\\n",
    "  --driver-memory 4g \\\n",
    "  --driver-memoryOverhead 1g \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Executor Memory Management\n",
    "\n",
    "### Three Main Sections:\n",
    "\n",
    "```text\n",
    "Total Executor Memory (Example: 4 GB)\n",
    "==========================================\n",
    "[Reserved Memory: 300 MB] - Fixed for Spark\n",
    "[Spark Memory Pool: 60%] - For Spark operations\n",
    "[User Memory: 40%] - For your code\n",
    "```\n",
    "\n",
    "### Spark Memory Pool (60% of total - 300MB)\n",
    "\n",
    "**Split into two parts:**\n",
    "\n",
    "**1. Storage Memory (50% of Spark Pool)**\n",
    "- Used for caching (cache(), persist())\n",
    "- Long-term storage\n",
    "- Data persists across operations\n",
    "\n",
    "**2. Execution Memory (50% of Spark Pool)**\n",
    "- Used for transformations (joins, sorts, aggregations)\n",
    "- Short-term, temporary\n",
    "- Released after operation completes\n",
    "\n",
    "```python\n",
    "# Configure executor memory\n",
    "spark-submit \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "# Default: 60% for Spark Pool\n",
    "spark.conf.set(\"spark.memory.fraction\", 0.6)\n",
    "\n",
    "# Default: 50% of Spark Pool for Storage\n",
    "spark.conf.set(\"spark.memory.storageFraction\", 0.5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Unified Memory Management\n",
    "\n",
    "**Key Feature:** Storage and Execution Memory can borrow from each other!\n",
    "\n",
    "```text\n",
    "Normal State:\n",
    "[Storage: 50%] [Execution: 50%]\n",
    "\n",
    "When Execution needs more:\n",
    "[Storage: 30%] [Execution: 70%] <- Borrowed!\n",
    "\n",
    "When Storage needs more:\n",
    "[Storage: 70%] [Execution: 30%] <- Borrowed!\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Dynamic allocation\n",
    "- Better memory utilization\n",
    "- Automatic optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Out of Memory (OOM) Errors\n",
    "\n",
    "### What Causes OOM?\n",
    "\n",
    "**Example Scenario:**\n",
    "```python\n",
    "# Huge groupBy on skewed data\n",
    "df.groupBy(\"ProductCategory\").count()\n",
    "\n",
    "# If \"Food\" category has 1M rows:\n",
    "# - Tries to fit in Execution Memory (1.2 GB)\n",
    "# - Memory fills up\n",
    "# - Borrows from Storage\n",
    "# - Still not enough\n",
    "# - Spills to disk\n",
    "# - Disk fills up\n",
    "# - OOM ERROR!\n",
    "```\n",
    "\n",
    "### Data Spill\n",
    "\n",
    "When memory is full, Spark writes to disk temporarily.\n",
    "\n",
    "**In Spark UI, look for:**\n",
    "- Spill (Memory): Amount spilled from memory\n",
    "- Spill (Disk): Amount written to disk\n",
    "\n",
    "**High spill = Performance problem!**\n",
    "\n",
    "---\n",
    "\n",
    "## Salting (Fix for Data Skew)\n",
    "\n",
    "### Problem:\n",
    "```text\n",
    "Product Categories:\n",
    "- Food: 1,000,000 rows (SKEWED!)\n",
    "- Shoes: 100 rows\n",
    "- Dairy: 200 rows\n",
    "\n",
    "Result: One executor overloaded, others idle\n",
    "```\n",
    "\n",
    "### Solution - Add Salt:\n",
    "```python\n",
    "from pyspark.sql.functions import rand, concat, lit\n",
    "\n",
    "# Add random salt (0-3)\n",
    "df_salted = df.withColumn(\"salt\", (rand() * 4).cast(\"int\"))\n",
    "df_salted = df_salted.withColumn(\"salted_key\", \n",
    "                                  concat(col(\"ProductCategory\"), lit(\"_\"), col(\"salt\")))\n",
    "\n",
    "# Now groupBy on salted key\n",
    "result = df_salted.groupBy(\"salted_key\").count()\n",
    "\n",
    "# Food is split into:\n",
    "# Food_0, Food_1, Food_2, Food_3\n",
    "# Distributed evenly across executors!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Caching & Persist\n",
    "\n",
    "### When to Cache?\n",
    "\n",
    "```python\n",
    "# BAD - Used only once\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df.count()  # No cache needed\n",
    "\n",
    "# GOOD - Used multiple times\n",
    "df = spark.read.parquet(\"/data\")\n",
    "df.cache()  # Cache it!\n",
    "\n",
    "df.filter(col(\"age\") > 30).count()  # Use 1\n",
    "df.filter(col(\"salary\") > 50000).count()  # Use 2\n",
    "df.groupBy(\"dept\").count().show()  # Use 3\n",
    "\n",
    "# Without cache: Reads /data 3 times\n",
    "# With cache: Reads /data once, reuses cached data\n",
    "```\n",
    "\n",
    "### cache() vs persist()\n",
    "\n",
    "```python\n",
    "# cache() is shortcut for MEMORY_AND_DISK\n",
    "df.cache()\n",
    "\n",
    "# persist() gives you control\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "```\n",
    "\n",
    "### Unpersist (Free Memory)\n",
    "\n",
    "```python\n",
    "df.cache()\n",
    "# ... use df ...\n",
    "df.unpersist()  # Free the memory!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Storage Levels\n",
    "\n",
    "### 1. MEMORY_ONLY\n",
    "- Fastest, but if memory is full, data is lost\n",
    "- Use when you have enough memory\n",
    "\n",
    "```python\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "```\n",
    "\n",
    "### 2. MEMORY_AND_DISK (Default)\n",
    "- Tries memory first, spills to disk if needed\n",
    "- Safe, balanced approach\n",
    "- Default for cache()\n",
    "\n",
    "```python\n",
    "df.cache()  # Uses MEMORY_AND_DISK\n",
    "```\n",
    "\n",
    "### 3. DISK_ONLY\n",
    "- Stores only on disk\n",
    "- Slow, rarely used\n",
    "\n",
    "```python\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "```\n",
    "\n",
    "### 4. MEMORY_ONLY_2\n",
    "- Stores in memory on TWO executors (replication)\n",
    "- Fault tolerant\n",
    "- Uses 2x memory\n",
    "\n",
    "```python\n",
    "# Critical data that must not be lost\n",
    "df.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "```\n",
    "\n",
    "### 5. OFF_HEAP\n",
    "- Stores outside JVM heap (in OS memory)\n",
    "- Avoids garbage collection pauses\n",
    "- For large executors (> 32 GB)\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.memory.offHeap.size\", \"10g\")\n",
    "\n",
    "df.persist(StorageLevel.OFF_HEAP)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Storage Levels Comparison\n",
    "\n",
    "| Storage Level | Memory | Disk | Speed | Use Case |\n",
    "|--------------|--------|------|-------|----------|\n",
    "| MEMORY_ONLY | Yes | No | Fastest | Small data, lots of RAM |\n",
    "| MEMORY_AND_DISK | Yes | Yes | Fast | Default, safe choice |\n",
    "| DISK_ONLY | No | Yes | Slow | Rarely used |\n",
    "| MEMORY_ONLY_2 | Yes (2x) | No | Fastest | Critical, fault-tolerant |\n",
    "| OFF_HEAP | Yes* | No | Fast | Large executors, avoid GC |\n",
    "\n",
    "---\n",
    "\n",
    "## Client Mode vs Cluster Mode\n",
    "\n",
    "### Client Mode\n",
    "- Driver runs on YOUR machine (laptop)\n",
    "- Good for development, notebooks\n",
    "- Your machine must stay connected\n",
    "\n",
    "```bash\n",
    "spark-submit --deploy-mode client app.py\n",
    "```\n",
    "\n",
    "**Visual:**\n",
    "```text\n",
    "Your Laptop (Driver)\n",
    "    |\n",
    "    v\n",
    "Cluster (Executors only)\n",
    "```\n",
    "\n",
    "**Use for:**\n",
    "- Development\n",
    "- Jupyter notebooks\n",
    "- Interactive analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster Mode\n",
    "- Driver runs ON the cluster\n",
    "- Production ready\n",
    "- Can submit job and disconnect\n",
    "\n",
    "```bash\n",
    "spark-submit --deploy-mode cluster app.py\n",
    "```\n",
    "\n",
    "**Visual:**\n",
    "```text\n",
    "Your Laptop (submit and disconnect)\n",
    "    |\n",
    "    v\n",
    "Cluster (Driver + Executors)\n",
    "```\n",
    "\n",
    "**Use for:**\n",
    "- Production jobs\n",
    "- Scheduled jobs\n",
    "- Long-running jobs\n",
    "\n",
    "---\n",
    "\n",
    "## Edge Node\n",
    "\n",
    "**What it is:**\n",
    "Gateway machine between you and the cluster.\n",
    "\n",
    "```text\n",
    "Your Laptop\n",
    "    |\n",
    "    | SSH\n",
    "    v\n",
    "Edge Node (Gateway)\n",
    "    |\n",
    "    v\n",
    "Hadoop/Spark Cluster\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "- Submit jobs to cluster\n",
    "- Security (don't expose cluster directly)\n",
    "- Centralized tools\n",
    "\n",
    "**Workflow:**\n",
    "```bash\n",
    "# SSH to edge node\n",
    "ssh user@edge-node.company.com\n",
    "\n",
    "# Submit job from edge node\n",
    "spark-submit --master yarn app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Partition Pruning\n",
    "\n",
    "**What it is:**\n",
    "Skip reading partitions that don't contain relevant data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Partitioned Data:**\n",
    "```python\n",
    "# Write partitioned by year\n",
    "df.write.partitionBy(\"year\").parquet(\"/sales\")\n",
    "\n",
    "# Directory structure:\n",
    "# /sales/year=2020/\n",
    "# /sales/year=2021/\n",
    "# /sales/year=2022/\n",
    "# /sales/year=2023/\n",
    "# /sales/year=2024/\n",
    "```\n",
    "\n",
    "**Query with Pruning:**\n",
    "```python\n",
    "df = spark.read.parquet(\"/sales\")\n",
    "result = df.filter(col(\"year\") == 2024)\n",
    "\n",
    "# Spark reads ONLY /sales/year=2024/\n",
    "# Skips 2020, 2021, 2022, 2023\n",
    "# Reads 20% of data instead of 100%!\n",
    "```\n",
    "\n",
    "**Performance Impact:**\n",
    "```text\n",
    "Without Pruning: Read 1 TB, filter to 200 GB (30 min)\n",
    "With Pruning: Read 200 GB directly (5 min)\n",
    "Savings: 6x faster!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## AQE (Adaptive Query Execution)\n",
    "\n",
    "**What it is:**\n",
    "Spark adjusts the query plan DURING execution based on real statistics.\n",
    "\n",
    "**Enable AQE:**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### Three Main Features:\n",
    "\n",
    "### 1. Dynamically Coalescing Partitions\n",
    "\n",
    "**Problem:**\n",
    "```python\n",
    "df = df.filter(col(\"active\") == True)  # 99% filtered out\n",
    "# Still has 1000 partitions, most are tiny!\n",
    "```\n",
    "\n",
    "**AQE Solution:**\n",
    "```text\n",
    "Detects many small partitions\n",
    "Merges them automatically\n",
    "1000 partitions -> 50 partitions\n",
    "Less overhead, better performance!\n",
    "```\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dynamically Switching Join Strategies\n",
    "\n",
    "**Problem:**\n",
    "```python\n",
    "# Estimated: 50 MB (shuffle join chosen)\n",
    "# Actual after filter: 5 MB (should broadcast!)\n",
    "```\n",
    "\n",
    "**AQE Solution:**\n",
    "```text\n",
    "Realizes actual size is small\n",
    "Switches to broadcast join mid-execution\n",
    "Much faster!\n",
    "```\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Dynamically Optimizing Skew Joins\n",
    "\n",
    "**Problem:**\n",
    "```text\n",
    "User \"popular\": 1M records (skewed!)\n",
    "Other users: 100 records each\n",
    "One executor overloaded!\n",
    "```\n",
    "\n",
    "**AQE Solution:**\n",
    "```text\n",
    "Detects skew\n",
    "Splits large partition\n",
    "Distributes across executors\n",
    "Balanced execution!\n",
    "```\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete AQE Configuration\n",
    "\n",
    "```python\n",
    "# Enable AQE (Spark 3.0+)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Coalesce partitions\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Auto broadcast\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "\n",
    "# Skew join\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "### Memory Management\n",
    "```python\n",
    "# Don't over-allocate\n",
    "# Rule: executor-memory = 4-8 GB per executor\n",
    "# More executors is better than huge executors\n",
    "\n",
    "spark-submit \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 20 \\\n",
    "  app.py\n",
    "```\n",
    "\n",
    "### Caching\n",
    "```python\n",
    "# Cache only what's reused\n",
    "df.cache()\n",
    "# ... use multiple times ...\n",
    "df.unpersist()  # Always unpersist when done!\n",
    "```\n",
    "\n",
    "### Storage Levels\n",
    "```python\n",
    "# For most cases\n",
    "df.cache()  # MEMORY_AND_DISK\n",
    "\n",
    "# For critical small data\n",
    "df.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "\n",
    "# For large executors\n",
    "df.persist(StorageLevel.OFF_HEAP)\n",
    "```\n",
    "\n",
    "### Partition Pruning\n",
    "```python\n",
    "# Always partition large tables\n",
    "df.write.partitionBy(\"date\", \"country\").parquet(\"/data\")\n",
    "\n",
    "# Filter on partition columns\n",
    "df.filter(col(\"date\") == \"2024-01-01\")  # Uses pruning!\n",
    "```\n",
    "\n",
    "### AQE\n",
    "```python\n",
    "# Enable in Spark 3.0+ (production default)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Issues & Solutions\n",
    "\n",
    "### Issue 1: OOM Errors\n",
    "**Solution:**\n",
    "- Increase executor memory\n",
    "- Reduce partition size (more partitions)\n",
    "- Use salting for skewed data\n",
    "- Avoid caching too much\n",
    "\n",
    "### Issue 2: Slow Performance with Cache\n",
    "**Solution:**\n",
    "- Use MEMORY_ONLY if you have RAM\n",
    "- Unpersist when done\n",
    "- Check if data actually fits in memory\n",
    "\n",
    "### Issue 3: Skewed Joins\n",
    "**Solution:**\n",
    "- Use salting technique\n",
    "- Enable AQE skew join optimization\n",
    "- Repartition by skewed key with salt\n",
    "\n",
    "### Issue 4: Too Many Small Files\n",
    "**Solution:**\n",
    "- Coalesce before writing\n",
    "- Use partition pruning\n",
    "- Enable AQE coalescing\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Memory Formula\n",
    "```text\n",
    "Executor Memory = Reserved (300MB) + Spark Pool (60%) + User (40%)\n",
    "Spark Pool = Storage (50%) + Execution (50%)\n",
    "```\n",
    "\n",
    "### Cache Decision\n",
    "```text\n",
    "Used once? -> No cache\n",
    "Used 2-3 times? -> Maybe cache\n",
    "Used 4+ times? -> Definitely cache\n",
    "```\n",
    "\n",
    "### Deployment Mode\n",
    "```text\n",
    "Development? -> Client mode\n",
    "Production? -> Cluster mode\n",
    "```\n",
    "\n",
    "### Storage Level\n",
    "```text\n",
    "Small, reused often? -> MEMORY_ONLY\n",
    "Default case? -> MEMORY_AND_DISK (cache())\n",
    "Critical data? -> MEMORY_ONLY_2\n",
    "Large executors? -> OFF_HEAP\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Memory Management:**\n",
    "- Understand executor memory layout\n",
    "- Monitor spills in Spark UI\n",
    "- Use salting for skewed data\n",
    "\n",
    "**Caching:**\n",
    "- Cache only reused data\n",
    "- Choose appropriate storage level\n",
    "- Unpersist when done\n",
    "\n",
    "**Deployment:**\n",
    "- Client mode for development\n",
    "- Cluster mode for production\n",
    "- Use edge nodes for security\n",
    "\n",
    "**Optimization:**\n",
    "- Use partition pruning (partition tables by common filters)\n",
    "- Enable AQE in Spark 3.0+\n",
    "- Monitor and tune based on metrics\n",
    "\n",
    "**Key Insight:** Proper memory management and caching can make your Spark jobs 10-100x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816cba1d-5bb1-489b-bc1e-46cc3399c69a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6550915559869987,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notes-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
